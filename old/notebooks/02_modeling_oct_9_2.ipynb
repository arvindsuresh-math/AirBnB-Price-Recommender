{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### **0. Setup and Installations**"
      ],
      "metadata": {
        "id": "o-mS-h3Ik30p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OY70zhGiktnZ",
        "outputId": "6ed09b25-6aea-4afa-eac9-d10c451bf09a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting Hugging Face login...\n",
            "Hugging Face login successful.\n"
          ]
        }
      ],
      "source": [
        "# --- Hugging Face Authentication (using Colab Secrets) ---\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "print(\"Attempting Hugging Face login...\")\n",
        "try:\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    login(token=HF_TOKEN)\n",
        "    print(\"Hugging Face login successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not log in. Please ensure 'HF_TOKEN' is a valid secret. Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Mount Google Drive ---\n",
        "from google.colab import drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not mount Google Drive. Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJLJ4mbMQOG0",
        "outputId": "2592385f-f40e-4e47-bfa3-31104974380a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Install Dependencies ---\n",
        "!pip install pandas\n",
        "!pip install pyarrow\n",
        "!pip install sentence-transformers\n",
        "!pip install scikit-learn\n",
        "!pip install torch\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQ2v1yVxk79e",
        "outputId": "9c7986bd-5f91-4df2-a357-fe9bda9bd3cc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (18.1.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.56.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. Configuration and Helper Functions**\n",
        "\n",
        "This section contains all hyperparameters and the new functions for data loading and splitting."
      ],
      "metadata": {
        "id": "1Kecda6zlEbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import json\n",
        "import time"
      ],
      "metadata": {
        "id": "7M2d2yhqlafF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Seeding function for reproducibility ---\n",
        "def set_seed(seed: int):\n",
        "    \"\"\"Sets the seed for all relevant RNGs to ensure reproducibility.\"\"\"\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed) # for multi-GPU\n",
        "        # These are crucial for reproducibility on GPU\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "    print(f\"All random seeds set to {seed}.\")\n",
        "\n",
        "class Config:\n",
        "    # --- Data and Environment ---\n",
        "    CITY: str = \"nyc\"\n",
        "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    DRIVE_SAVE_PATH: str = \"/content/drive/MyDrive/Colab_Notebooks/Airbnb_Project/\"\n",
        "\n",
        "    # --- Data Pre-processing ---\n",
        "    VAL_SIZE: float = 0.2\n",
        "\n",
        "    # --- Reproducibility ---\n",
        "    SEED: int = 42 # Master seed for the entire experiment\n",
        "\n",
        "    # --- Model Training ---\n",
        "    BATCH_SIZE: int = 1024\n",
        "    LEARNING_RATE: float = 1e-3\n",
        "    N_EPOCHS: int = 30\n",
        "\n",
        "    # --- Early Stopping ---\n",
        "    EARLY_STOPPING_PATIENCE: int = 5\n",
        "    EARLY_STOPPING_MIN_DELTA: float = 1e-4 # For RW-MSLE, the accuracy metric\n",
        "\n",
        "    # --- Logging ---\n",
        "    LOG_EVERY_N_STEPS: int = 10"
      ],
      "metadata": {
        "id": "9GzwsZ61lGJi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Data Loading and Splitting**\n",
        "\n",
        "This function handles loading, outlier removal, and the 3-way stratified split."
      ],
      "metadata": {
        "id": "zpZgvwABtJbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_split_data(config: Config):\n",
        "    \"\"\"\n",
        "    Loads data, removes price outliers, and performs a 3-way stratified split.\n",
        "    \"\"\"\n",
        "    dataset_filename = f\"{config.CITY}_final_modeling_dataset.parquet\"\n",
        "    dataset_path = f\"./{dataset_filename}\" # Assumes file in root Colab runtime\n",
        "\n",
        "    if not os.path.exists(dataset_path):\n",
        "        raise FileNotFoundError(f\"'{dataset_filename}' not found. Please upload the file to the Colab Runtime.\")\n",
        "\n",
        "    print(f\"Loading dataset from: {dataset_path}\")\n",
        "    df = pd.read_parquet(dataset_path)\n",
        "\n",
        "    # Remove price outliers (top/bottom 1%)\n",
        "    price_q01 = df['target_price'].quantile(0.01)\n",
        "    price_q99 = df['target_price'].quantile(0.99)\n",
        "    df = df[(df['target_price'] >= price_q01) & (df['target_price'] <= price_q99)].copy()\n",
        "    print(f\"Removed price outliers. New size: {len(df):,} records.\")\n",
        "\n",
        "    # Create bins for stratifying continuous price\n",
        "    df['price_bin'] = pd.cut(df['target_price'], bins=10, labels=False)\n",
        "\n",
        "    # Create a combined key for 3-way stratification\n",
        "    stratify_key = (\n",
        "        df['neighbourhood_cleansed'].astype(str) + '_' +\n",
        "        df['month'].astype(str) + '_' +\n",
        "        df['price_bin'].astype(str)\n",
        "    )\n",
        "\n",
        "    # Handle small strata (<2 members)\n",
        "    strata_counts = stratify_key.value_counts()\n",
        "    valid_strata = strata_counts[strata_counts >= 2].index\n",
        "    df_filtered = df[stratify_key.isin(valid_strata)].copy()\n",
        "    print(f\"Removed small strata. New size: {len(df_filtered):,} records.\")\n",
        "\n",
        "    # Perform stratified split\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        df_filtered.index,\n",
        "        test_size=config.VAL_SIZE,\n",
        "        random_state=config.SEED,\n",
        "        stratify=stratify_key[df_filtered.index]\n",
        "    )\n",
        "\n",
        "    train_df = df_filtered.loc[train_indices].copy().reset_index(drop=True)\n",
        "    val_df = df_filtered.loc[val_indices].copy().reset_index(drop=True)\n",
        "\n",
        "    print(f\"Split complete. Training: {len(train_df):,}, Validation: {len(val_df):,}\")\n",
        "\n",
        "    print(\"\\n--- Sample Record from Training Data ---\")\n",
        "    # Pretty-print the first record by transposing it\n",
        "    print(train_df.head(1).T)\n",
        "\n",
        "    return train_df, val_df"
      ],
      "metadata": {
        "id": "kszCgSb1lPWV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Feature Processor**\n",
        "\n",
        "This class encapsulates the feature engineering logic as defined in `EMBEDDINGS.md`. It learns transformations (like vocabularies and scaling parameters) from the training data via the `.fit()` method. The `.transform()` method then consistently applies these learned transformations to any dataset, preventing data leakage. This is a crucial step for creating model-ready tensors from raw dataframes."
      ],
      "metadata": {
        "id": "IOVmfA44lizn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureProcessor:\n",
        "    def __init__(self, embedding_dim_geo: int = 32):\n",
        "        self.vocabs, self.scalers = {}, {}\n",
        "        self.embedding_dim_geo = embedding_dim_geo\n",
        "        self.categorical_cols = [\n",
        "            \"neighbourhood_cleansed\",\n",
        "            \"property_type\",\n",
        "            \"room_type\",\n",
        "            \"bathrooms_type\",\n",
        "            \"bedrooms\",\n",
        "            \"beds\",\n",
        "            \"bathrooms_numeric\"\n",
        "            ]\n",
        "        self.numerical_cols = [\n",
        "            \"accommodates\",\n",
        "            \"review_scores_rating\",\n",
        "            \"review_scores_cleanliness\",\n",
        "            \"review_scores_checkin\",\n",
        "            \"review_scores_communication\",\n",
        "            \"review_scores_location\",\n",
        "            \"review_scores_value\",\n",
        "            \"host_response_rate\",\n",
        "            \"host_acceptance_rate\"\n",
        "            ]\n",
        "        self.log_transform_cols = [\"number_of_reviews_ltm\"]\n",
        "        self.boolean_cols = [\n",
        "            \"host_is_superhost\",\n",
        "            \"host_identity_verified\",\n",
        "            \"instant_bookable\"\n",
        "            ]\n",
        "\n",
        "    def _create_positional_encoding(self, value, max_val):\n",
        "        d = self.embedding_dim_geo\n",
        "        if d % 2 != 0: raise ValueError(\"embedding_dim_geo must be even.\")\n",
        "        pe = np.zeros(d)\n",
        "        position = (value / max_val) * 10000\n",
        "        div_term = np.exp(np.arange(0, d, 2) * -(np.log(10000.0) / d))\n",
        "        pe[0::2] = np.sin(position * div_term)\n",
        "        pe[1::2] = np.cos(position * div_term)\n",
        "        return pe\n",
        "\n",
        "    def fit(self, df: pd.DataFrame):\n",
        "        for col in self.categorical_cols:\n",
        "            valid_uniques = df[col].dropna().unique().tolist()\n",
        "            self.vocabs[col] = {val: i for i, val in enumerate([\"<UNK>\"] + sorted(valid_uniques))}\n",
        "        for col in self.numerical_cols + self.log_transform_cols:\n",
        "            vals = np.log1p(df[col]) if col in self.log_transform_cols else df[col]\n",
        "            self.scalers[col] = {'mean': vals.mean(), 'std': vals.std()}\n",
        "\n",
        "    def transform(self, df: pd.DataFrame) -> dict:\n",
        "        df = df.copy()\n",
        "        half_dim = self.embedding_dim_geo // 2\n",
        "        lat_enc = df['latitude'].apply(lambda x: self._create_positional_encoding(x, 90)[:half_dim])\n",
        "        lon_enc = df['longitude'].apply(lambda x: self._create_positional_encoding(x, 180)[:half_dim])\n",
        "\n",
        "        # --- Axis 1: Location ---\n",
        "        half_dim = self.embedding_dim_geo // 2\n",
        "        lat_enc = df['latitude'].apply(lambda x: self._create_positional_encoding(x, 90)[:half_dim])\n",
        "        lon_enc = df['longitude'].apply(lambda x: self._create_positional_encoding(x, 180)[:half_dim])\n",
        "        geo_position = np.hstack([np.stack(lat_enc), np.stack(lon_enc)])\n",
        "        neighbourhood = df[\"neighbourhood_cleansed\"].apply(lambda x: self.vocabs[\"neighbourhood_cleansed\"].get(x, 0)).values\n",
        "        location_features = {\"geo_position\": geo_position, \"neighbourhood\": neighbourhood}\n",
        "\n",
        "        # --- Axis 2: Size & Capacity ---\n",
        "        size_features = {}\n",
        "        for col in [\"property_type\", \"room_type\", \"bathrooms_type\", \"bedrooms\", \"beds\", \"bathrooms_numeric\"]:\n",
        "            size_features[col] = df[col].apply(lambda x: self.vocabs[col].get(x, 0) if pd.notna(x) else 0).values\n",
        "        size_features[\"accommodates\"] = ((df[\"accommodates\"] - self.scalers[\"accommodates\"][\"mean\"]) / self.scalers[\"accommodates\"][\"std\"]).values\n",
        "\n",
        "        # --- Axis 3: Quality & Reputation ---\n",
        "        quality_features = {}\n",
        "        for col in self.numerical_cols:\n",
        "            if col != \"accommodates\":\n",
        "                quality_features[col] = ((df[col] - self.scalers[col][\"mean\"]) / self.scalers[col][\"std\"]).values\n",
        "        quality_features[\"number_of_reviews_ltm\"] = ((np.log1p(df[\"number_of_reviews_ltm\"]) - self.scalers[\"number_of_reviews_ltm\"][\"mean\"]) / self.scalers[\"number_of_reviews_ltm\"][\"std\"]).values\n",
        "        for col in self.boolean_cols:\n",
        "            quality_features[col] = df[col].astype(float).values\n",
        "\n",
        "        # --- Axis 5: Seasonality ---\n",
        "        month_sin = np.sin(2 * np.pi * df[\"month\"] / 12)\n",
        "        month_cos = np.cos(2 * np.pi * df[\"month\"] / 12)\n",
        "        seasonality_features = {\"cyclical\": np.vstack([month_sin, month_cos]).T}\n",
        "\n",
        "        return {\n",
        "            \"location\": location_features,\n",
        "            \"size_capacity\": size_features,\n",
        "            \"quality\": quality_features,\n",
        "            \"amenities\": {\"text\": df[\"amenities\"].tolist()},\n",
        "            \"seasonality\": seasonality_features,\n",
        "            \"target_price\": df[\"target_price\"].values, # try without log to see if training is stable\n",
        "            \"sample_weight\": df[\"estimated_occupancy_rate\"].values\n",
        "        }"
      ],
      "metadata": {
        "id": "v2tlR__3lsm8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. AirbnbDataset Class**\n",
        "\n",
        "The PyTorch `Dataset` class, which defines how to retrieve a single item from our processed feature dictionary."
      ],
      "metadata": {
        "id": "oifKuGPcnah9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AirbnbPriceDataset(Dataset):\n",
        "    def __init__(self, features: dict):\n",
        "        self.features = features\n",
        "        self.n_samples = len(features['sample_weight'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "    def __getitem__(self, index: int) -> dict:\n",
        "        item = {}\n",
        "        # Location\n",
        "        item['loc_geo_position'] = self.features['location']['geo_position'][index]\n",
        "        item['loc_neighbourhood'] = self.features['location']['neighbourhood'][index]\n",
        "\n",
        "        # Size & Capacity\n",
        "        for k, v in self.features['size_capacity'].items():\n",
        "            item[f'size_{k}'] = v[index]\n",
        "\n",
        "        # Quality\n",
        "        for k, v in self.features['quality'].items():\n",
        "            item[f'qual_{k}'] = v[index]\n",
        "\n",
        "        # Amenities & Seasonality\n",
        "        item['amenities_text'] = self.features['amenities']['text'][index]\n",
        "        item['season_cyclical'] = self.features['seasonality']['cyclical'][index]\n",
        "\n",
        "        # Target & Weight\n",
        "        item['target'] = self.features['target_price'][index]\n",
        "        item['sample_weight'] = self.features['sample_weight'][index]\n",
        "\n",
        "        return item"
      ],
      "metadata": {
        "id": "HekaBx8inlWH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Dataloader Creation**\n",
        "\n",
        "A function to create the `DataLoader` instances, including the custom collate function for batch tokenization."
      ],
      "metadata": {
        "id": "rti3lqEqvXGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (This cell replaces the 'preprocess_and_tensorize' and 'create_dataloaders' functions)\n",
        "\n",
        "def preprocess_and_tensorize_CPU(processor, df):\n",
        "    \"\"\"\n",
        "    Applies the feature processor and converts data to CPU tensors with correct dtypes.\n",
        "    \"\"\"\n",
        "    features_cpu = processor.transform(df)\n",
        "    features_tensor = {}\n",
        "\n",
        "    for key, value in features_cpu.items():\n",
        "        if key == 'amenities':\n",
        "            features_tensor[key] = value # Keep raw text\n",
        "        elif isinstance(value, dict):\n",
        "            features_tensor[key] = {}\n",
        "            for sub_key, sub_val in value.items():\n",
        "                # --- CORRECTED DTYPE LOGIC ---\n",
        "                # Use the lists from the processor to determine the correct dtype.\n",
        "                # Categorical features need to be 'long' for embedding layers.\n",
        "                # All other features (numerical, boolean, cyclical, positional) must be 'float'.\n",
        "                if sub_key in processor.categorical_cols or sub_key == 'neighbourhood':\n",
        "                    dtype = torch.long\n",
        "                else:\n",
        "                    dtype = torch.float32\n",
        "\n",
        "                features_tensor[key][sub_key] = torch.from_numpy(sub_val).to(dtype=dtype)\n",
        "        else: # Handles top-level items like target_log_price and sample_weight\n",
        "            features_tensor[key] = torch.from_numpy(value).to(dtype=torch.float32)\n",
        "\n",
        "    return features_tensor\n",
        "\n",
        "def create_dataloaders(train_features_cpu, val_features_cpu, config: Config):\n",
        "    \"\"\"Creates high-performance, reproducible PyTorch DataLoaders.\"\"\"\n",
        "\n",
        "    # Tokenizer can still be on the GPU for speed, as it's used in the main process\n",
        "    tokenizer_model = SentenceTransformer('BAAI/bge-small-en-v1.5', device=config.DEVICE)\n",
        "\n",
        "    def custom_collate_fn(batch: list) -> dict:\n",
        "        amenities_texts = [item.pop('amenities_text') for item in batch]\n",
        "        collated_batch = {key: torch.stack([d[key] for d in batch]) for key in batch[0].keys()}\n",
        "        # The tokenizer is called, but its output remains on CPU by default\n",
        "        tokenized = tokenizer_model.tokenizer(\n",
        "            amenities_texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors='pt',\n",
        "            max_length=128\n",
        "        )\n",
        "        collated_batch['amenities_tokens'] = tokenized\n",
        "        return collated_batch\n",
        "\n",
        "    train_dataset = AirbnbPriceDataset(train_features_cpu)\n",
        "    val_dataset = AirbnbPriceDataset(val_features_cpu)\n",
        "\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(config.SEED)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=config.BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        collate_fn=custom_collate_fn,\n",
        "        generator=g,\n",
        "        pin_memory=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        dataset=val_dataset,\n",
        "        batch_size=config.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        collate_fn=custom_collate_fn,\n",
        "        pin_memory=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "    print(f\"DataLoaders created with pin_memory=True and num_workers=2.\")\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "xSMyFuNHnmQu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Model Architecture**\n",
        "\n",
        "This is the `AdditiveAxisModel`, our core neural network. As detailed in `MODELING.md`, it's a multi-headed architecture where each \"head\" or sub-network is responsible for a distinct feature axis (Location, Size, etc.). The final price is the sum of contributions from each axis plus a global bias. This design makes the model's predictions inherently explainable."
      ],
      "metadata": {
        "id": "GBPciJiSnyF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridExplainableModel(nn.Module):\n",
        "    def __init__(self, processor: FeatureProcessor, device: str):\n",
        "        super().__init__()\n",
        "        self.vocabs, self.device = processor.vocabs, device\n",
        "\n",
        "        # --- Embedding Layers (unchanged) ---\n",
        "        self.embed_neighbourhood = nn.Embedding(len(self.vocabs['neighbourhood_cleansed']), 16)\n",
        "        self.embed_property_type = nn.Embedding(len(self.vocabs['property_type']), 8)\n",
        "        self.embed_room_type = nn.Embedding(len(self.vocabs['room_type']), 4)\n",
        "        self.embed_bathrooms_type = nn.Embedding(len(self.vocabs['bathrooms_type']), 2)\n",
        "        self.embed_bedrooms = nn.Embedding(len(self.vocabs['bedrooms']), 4)\n",
        "        self.embed_beds = nn.Embedding(len(self.vocabs['beds']), 4)\n",
        "        self.embed_bathrooms_numeric = nn.Embedding(len(self.vocabs['bathrooms_numeric']), 4)\n",
        "        self.amenities_transformer = SentenceTransformer('BAAI/bge-small-en-v1.5', device=self.device)\n",
        "        for param in self.amenities_transformer.parameters(): param.requires_grad = False\n",
        "\n",
        "        # --- 1. Attribution Heads (formerly subnets) ---\n",
        "        # These predict the additive dollar contributions\n",
        "        self.loc_head = nn.Sequential(nn.Linear(48, 32), nn.ReLU(), nn.Linear(32, 1))\n",
        "        self.size_head = nn.Sequential(nn.Linear(27, 32), nn.ReLU(), nn.Linear(32, 1))\n",
        "        self.qual_head = nn.Sequential(nn.Linear(12, 32), nn.ReLU(), nn.Linear(32, 1))\n",
        "        self.amenities_head = nn.Linear(384, 1)\n",
        "        self.season_head = nn.Sequential(nn.Linear(2, 16), nn.ReLU(), nn.Linear(16, 1))\n",
        "        self.global_bias_head = nn.Parameter(torch.randn(1))\n",
        "\n",
        "        # --- 2. Backbone (for accurate prediction) ---\n",
        "        # It takes all features concatenated together\n",
        "        backbone_input_dim = 48 + 27 + 12 + 384 + 2 # loc+size+qual+amen+season\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Linear(backbone_input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, batch: dict) -> dict:\n",
        "        # --- Create feature embeddings (same as before) ---\n",
        "        loc_geo = batch['loc_geo_position']\n",
        "        loc_hood_embed = self.embed_neighbourhood(batch['loc_neighbourhood'])\n",
        "        loc_input = torch.cat([loc_geo, loc_hood_embed], dim=1)\n",
        "        size_embeds = [\n",
        "            self.embed_property_type(batch['size_property_type']), self.embed_room_type(batch['size_room_type']),\n",
        "            self.embed_bathrooms_type(batch['size_bathrooms_type']), self.embed_beds(batch['size_beds']),\n",
        "            self.embed_bedrooms(batch['size_bedrooms']), self.embed_bathrooms_numeric(batch['size_bathrooms_numeric']),\n",
        "            batch['size_accommodates'].unsqueeze(1)\n",
        "        ]\n",
        "        size_input = torch.cat(size_embeds, dim=1)\n",
        "        qual_inputs = [\n",
        "            batch[f'qual_{col}'].unsqueeze(1) for col in [\n",
        "                \"review_scores_rating\", \"review_scores_cleanliness\", \"review_scores_checkin\",\n",
        "                \"review_scores_communication\", \"review_scores_location\", \"review_scores_value\",\n",
        "                \"host_response_rate\", \"host_acceptance_rate\", \"number_of_reviews_ltm\",\n",
        "                \"host_is_superhost\", \"host_identity_verified\", \"instant_bookable\"\n",
        "            ]\n",
        "        ]\n",
        "        qual_input = torch.cat(qual_inputs, dim=1)\n",
        "        amenities_tokens = batch['amenities_tokens'].to(self.device)\n",
        "        amenities_input = self.amenities_transformer(amenities_tokens)['sentence_embedding']\n",
        "        season_input = batch['season_cyclical']\n",
        "\n",
        "        # --- Backbone Prediction (for accuracy) ---\n",
        "        backbone_full_input = torch.cat([\n",
        "            loc_input, size_input, qual_input, amenities_input, season_input\n",
        "        ], dim=1)\n",
        "        q_pred = self.backbone(backbone_full_input).squeeze(-1)\n",
        "\n",
        "        # --- Attribution Head Predictions (for explanation) ---\n",
        "        p_loc = self.loc_head(loc_input)\n",
        "        p_size = self.size_head(size_input)\n",
        "        p_qual = self.qual_head(qual_input)\n",
        "        p_amenities = self.amenities_head(amenities_input)\n",
        "        p_season = self.season_head(season_input)\n",
        "\n",
        "        return {\n",
        "            \"q_pred\": q_pred, # Predicted log_price\n",
        "            \"p_loc\": p_loc.squeeze(-1),\n",
        "            \"p_size\": p_size.squeeze(-1),\n",
        "            \"p_qual\": p_qual.squeeze(-1),\n",
        "            \"p_amenities\": p_amenities.squeeze(-1),\n",
        "            \"p_season\": p_season.squeeze(-1),\n",
        "            \"p_bias\": self.global_bias_head.expand(q_pred.shape[0])\n",
        "        }"
      ],
      "metadata": {
        "id": "fPe4QfN0n01P"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7. Training Function**\n",
        "\n",
        "This function orchestrates the training and validation loops for a given number of epochs.\n"
      ],
      "metadata": {
        "id": "EdFvzcyOn4yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_loader, device, config):\n",
        "    \"\"\"Runs a full evaluation pass for the hybrid model.\"\"\"\n",
        "    model.eval()\n",
        "    total_backbone_loss = 0.0\n",
        "    total_attribution_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            for key, value in batch.items():\n",
        "                if isinstance(value, torch.Tensor): batch[key] = value.to(device, non_blocking=True)\n",
        "                elif isinstance(value, dict):\n",
        "                    for sub_key, sub_value in value.items(): batch[key][sub_key] = sub_value.to(device, non_blocking=True)\n",
        "\n",
        "            targets = batch['target']\n",
        "            weights = batch['sample_weight']\n",
        "\n",
        "            # Get all predictions\n",
        "            preds = model(batch)\n",
        "            q_pred = preds['q_pred']\n",
        "\n",
        "            # --- Calculate Backbone Loss (Accuracy) ---\n",
        "            target_log_price = torch.log1p(targets)\n",
        "            backbone_loss = (weights * (q_pred - target_log_price)**2).mean().item()\n",
        "            total_backbone_loss += backbone_loss\n",
        "\n",
        "            # --- Calculate Attribution Loss (Explanation) ---\n",
        "            p_pred = torch.exp(q_pred)\n",
        "            p_sum = preds['p_bias'] + preds['p_loc'] + preds['p_size'] + preds['p_qual'] + preds['p_amenities'] + preds['p_season']\n",
        "            attribution_loss = (weights * (p_sum - p_pred)**2).mean().item()\n",
        "            total_attribution_loss += attribution_loss\n",
        "\n",
        "    return {\n",
        "        \"backbone_loss\": total_backbone_loss / len(data_loader),\n",
        "        \"attribution_loss\": total_attribution_loss / len(data_loader)\n",
        "    }\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, config):\n",
        "    \"\"\"\n",
        "    Trains the hybrid model, logging both training and validation losses for each component.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Model Training (Hybrid Architecture) ---\")\n",
        "    start_time = time.time()\n",
        "    history = []\n",
        "    global_step_count = 0\n",
        "    patience_counter = 0\n",
        "    early_stop_flag = False\n",
        "\n",
        "    print(\"Performing Step 0 evaluation...\")\n",
        "    # Get initial validation losses\n",
        "    val_losses_0 = evaluate_model(model, val_loader, config.DEVICE, config)\n",
        "    # Get initial training losses (on first batch for speed)\n",
        "    first_train_batch = next(iter(train_loader))\n",
        "    train_losses_0 = evaluate_model(model, [first_train_batch], config.DEVICE, config)\n",
        "\n",
        "    best_val_loss = val_losses_0['backbone_loss'] # Early stopping is based on accuracy\n",
        "    best_model_state = model.state_dict()\n",
        "    print(\"Step 0 evaluation complete.\\n\")\n",
        "\n",
        "    # --- Updated Header ---\n",
        "    header = (\n",
        "        f\"{'Steps':>5} | {'Epoch':>5} | {'Train RW-MSLE':>13} | {'Val RW-MSLE':>11} | \"\n",
        "        f\"{'Train Attr RMSE':>15} | {'Val Attr RMSE':>13} | {'Patience':>8} | {'Elapsed Time'}\"\n",
        "    )\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "\n",
        "    # --- Log Step 0 ---\n",
        "    elapsed_time_str = time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))\n",
        "    step_stats_0 = {\n",
        "        'Steps': 0, 'Epoch': 0.00,\n",
        "        'Train RW-MSLE': np.sqrt(train_losses_0['backbone_loss']),\n",
        "        'Val RW-MSLE': np.sqrt(val_losses_0['backbone_loss']),\n",
        "        'Train Attr RMSE': np.sqrt(train_losses_0['attribution_loss']),\n",
        "        'Val Attr RMSE': np.sqrt(val_losses_0['attribution_loss']),\n",
        "        'Patience': 0, 'Elapsed Time': elapsed_time_str\n",
        "    }\n",
        "    history.append(step_stats_0)\n",
        "\n",
        "    log_line_0 = (\n",
        "        f\"{0:>5d} | {0:>5.2f} | {step_stats_0['Train RW-MSLE']:>13.4f} | {step_stats_0['Val RW-MSLE']:>11.4f} | \"\n",
        "        f\"{step_stats_0['Train Attr RMSE']:>15.2f} | {step_stats_0['Val Attr RMSE']:>13.2f} | \"\n",
        "        f\"{0:>8d} | {elapsed_time_str}\"\n",
        "    )\n",
        "    print(log_line_0)\n",
        "\n",
        "    for epoch in range(config.N_EPOCHS):\n",
        "        if early_stop_flag: break\n",
        "        model.train()\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            if early_stop_flag: break\n",
        "\n",
        "            for key, value in batch.items():\n",
        "                if isinstance(value, torch.Tensor): batch[key] = value.to(config.DEVICE, non_blocking=True)\n",
        "                elif isinstance(value, dict):\n",
        "                    for sub_key, sub_value in value.items(): batch[key][sub_key] = sub_value.to(config.DEVICE, non_blocking=True)\n",
        "\n",
        "            targets = batch['target']\n",
        "            weights = batch['sample_weight']\n",
        "            preds = model(batch)\n",
        "            q_pred = preds['q_pred']\n",
        "\n",
        "            target_log_price = torch.log1p(targets)\n",
        "            loss_backbone = (weights * (q_pred - target_log_price)**2).mean()\n",
        "\n",
        "            p_pred = torch.exp(q_pred).detach()\n",
        "            p_sum = preds['p_bias'] + preds['p_loc'] + preds['p_size'] + preds['p_qual'] + preds['p_amenities'] + preds['p_season']\n",
        "            loss_attribution = (weights * (p_sum - p_pred)**2).mean()\n",
        "\n",
        "            total_loss = loss_backbone + loss_attribution\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            global_step_count += 1\n",
        "\n",
        "            if (global_step_count % config.LOG_EVERY_N_STEPS == 0):\n",
        "                val_losses = evaluate_model(model, val_loader, config.DEVICE, config)\n",
        "                current_val_loss = val_losses['backbone_loss']\n",
        "\n",
        "                if current_val_loss < best_val_loss - config.EARLY_STOPPING_MIN_DELTA:\n",
        "                    best_val_loss = current_val_loss\n",
        "                    patience_counter = 0\n",
        "                    best_model_state = model.state_dict()\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "\n",
        "                if patience_counter >= config.EARLY_STOPPING_PATIENCE:\n",
        "                    early_stop_flag = True\n",
        "\n",
        "                elapsed_time_str = time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))\n",
        "\n",
        "                step_stats = {\n",
        "                    'Steps': global_step_count,\n",
        "                    'Epoch': epoch + (i + 1) / len(train_loader),\n",
        "                    'Train RW-MSLE': np.sqrt(loss_backbone.item()), # Current batch train loss\n",
        "                    'Val RW-MSLE': np.sqrt(current_val_loss),\n",
        "                    'Train Attr RMSE': np.sqrt(loss_attribution.item()), # Current batch train loss\n",
        "                    'Val Attr RMSE': np.sqrt(val_losses['attribution_loss']),\n",
        "                    'Patience': patience_counter,\n",
        "                    'Elapsed Time': elapsed_time_str\n",
        "                }\n",
        "                history.append(step_stats)\n",
        "\n",
        "                log_line = (\n",
        "                    f\"{global_step_count:>5d} | {step_stats['Epoch']:>5.2f} | \"\n",
        "                    f\"{step_stats['Train RW-MSLE']:>13.4f} | {step_stats['Val RW-MSLE']:>11.4f} | \"\n",
        "                    f\"{step_stats['Train Attr RMSE']:>15.2f} | {step_stats['Val Attr RMSE']:>13.2f} | \"\n",
        "                    f\"{patience_counter:>8d} | {elapsed_time_str}\"\n",
        "                )\n",
        "                print(log_line)\n",
        "\n",
        "                if early_stop_flag:\n",
        "                    print(f\"\\n--- Early Stopping Triggered at Step {global_step_count} ---\")\n",
        "\n",
        "    print(\"\\n--- Training Complete ---\")\n",
        "    if best_model_state is not None:\n",
        "        print(f\"Loading best model state (Val RW-MSLE: {np.sqrt(best_val_loss):.4f})\")\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    return model, pd.DataFrame(history)"
      ],
      "metadata": {
        "id": "I5L0rVz6n4Fh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8. Main Execution Function**\n",
        "\n",
        "This single cell runs the entire pipeline from start to finish using the settings defined in the `Config` class."
      ],
      "metadata": {
        "id": "joZIdabe1wA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_artifacts(artifacts: dict, config: Config):\n",
        "    \"\"\"Saves the essential training artifacts to a single file.\"\"\"\n",
        "    timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
        "    filename = f\"{config.CITY}_artifacts_{timestamp}.pt\"\n",
        "\n",
        "    # Define paths for local runtime and Google Drive\n",
        "    runtime_path = f\"./{filename}\"\n",
        "    drive_path = os.path.join(config.DRIVE_SAVE_PATH, filename)\n",
        "\n",
        "    # Ensure Google Drive directory exists\n",
        "    os.makedirs(config.DRIVE_SAVE_PATH, exist_ok=True)\n",
        "\n",
        "    print(f\"\\nSaving artifacts to {runtime_path} and {drive_path}...\")\n",
        "    torch.save(artifacts, runtime_path)\n",
        "    torch.save(artifacts, drive_path)\n",
        "    print(\"Artifacts saved successfully.\")\n",
        "\n",
        "def main(config: Config):\n",
        "    \"\"\"Runs the end-to-end training pipeline for the Hybrid Explainable Model.\"\"\"\n",
        "    # 1. Load and split data\n",
        "    train_df, val_df = load_and_split_data(config)\n",
        "\n",
        "    # 2. Process features\n",
        "    processor = FeatureProcessor()\n",
        "    processor.fit(train_df)\n",
        "    train_features_cpu = preprocess_and_tensorize_CPU(processor, train_df)\n",
        "    val_features_cpu = preprocess_and_tensorize_CPU(processor, val_df)\n",
        "\n",
        "    # 3. Create DataLoaders\n",
        "    train_loader, val_loader = create_dataloaders(train_features_cpu, val_features_cpu, config)\n",
        "\n",
        "    # 4. Initialize the new HybridExplainableModel and optimizer\n",
        "    model = HybridExplainableModel(processor, device=config.DEVICE)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=config.LEARNING_RATE)\n",
        "\n",
        "    # 5. Run training\n",
        "    trained_model, training_history = train_model(model, train_loader, val_loader, optimizer, config)\n",
        "\n",
        "    # 6. Collate all artifacts into a single dictionary\n",
        "    artifacts = {\n",
        "        \"config\": config,\n",
        "        \"processor\": processor,\n",
        "        \"model_state_dict\": trained_model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"history\": training_history\n",
        "    }\n",
        "\n",
        "    # 7. Save artifacts and display final results\n",
        "    save_artifacts(artifacts, config)\n",
        "\n",
        "    print(\"\\n--- Final Training History ---\")\n",
        "    if not training_history.empty:\n",
        "        display_format = {\n",
        "            'Epoch': '{:.2f}'.format,\n",
        "            'Train RW-MSLE': '{:.4f}'.format,\n",
        "            'Val RW-MSLE': '{:.4f}'.format,\n",
        "            'Train Attr RMSE': '{:.2f}'.format,\n",
        "            'Val Attr RMSE': '{:.2f}'.format,\n",
        "        }\n",
        "        display(training_history.set_index('Steps').style.format(display_format))\n",
        "\n",
        "    return artifacts"
      ],
      "metadata": {
        "id": "MX6c4H56n9pZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **9. Final execution cell**\n",
        "\n",
        "Requires two steps-- First, instantiate a Config object (`config`, say), changing any attributes from the default as needed. Next, simply run `main(config)`"
      ],
      "metadata": {
        "id": "q6dclfe22Hka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the configuration\n",
        "config = Config()\n",
        "set_seed(config.SEED)\n",
        "\n",
        "print(f\"Configuration loaded:\")\n",
        "print(f\"Device: {config.DEVICE}\")\n",
        "print(f\"City: {config.CITY}\")\n",
        "print(f\"Seed: {config.SEED}\")\n",
        "print(f\"Batch Size: {config.BATCH_SIZE}\")\n",
        "print(f\"Learning Rate: {config.LEARNING_RATE}\")\n",
        "print(f\"Number of Epochs: {config.N_EPOCHS}\")\n",
        "print(f\"Logging Interval (steps): {config.LOG_EVERY_N_STEPS}\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ALhD09_2XJz",
        "outputId": "085d49bf-75ed-47fe-d08e-10b68737607c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All random seeds set to 42.\n",
            "Configuration loaded:\n",
            "Device: cuda\n",
            "City: nyc\n",
            "Seed: 42\n",
            "Batch Size: 1024\n",
            "Learning Rate: 0.001\n",
            "Number of Epochs: 30\n",
            "Logging Interval (steps): 10\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the end-to-end training pipeline\n",
        "training_artifacts = main(config)"
      ],
      "metadata": {
        "id": "zxhFDl2oZsFC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b7d98ef7-3d9b-41a6-a893-d768671b4f36"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset from: ./nyc_final_modeling_dataset.parquet\n",
            "Removed price outliers. New size: 81,643 records.\n",
            "Removed small strata. New size: 79,485 records.\n",
            "Split complete. Training: 63,588, Validation: 15,897\n",
            "\n",
            "--- Sample Record from Training Data ---\n",
            "                                                                             0\n",
            "listing_id                                                  779010937952266773\n",
            "year_month                                                             2024-11\n",
            "target_price                                                              90.0\n",
            "estimated_occupancy_rate                                              0.066667\n",
            "latitude                                                              40.63478\n",
            "longitude                                                             -73.9501\n",
            "neighbourhood_cleansed                                                Flatbush\n",
            "property_type                                               Entire rental unit\n",
            "room_type                                                      Entire home/apt\n",
            "accommodates                                                                 2\n",
            "bedrooms                                                                   1.0\n",
            "beds                                                                       1.0\n",
            "bathrooms_numeric                                                          1.0\n",
            "bathrooms_type                                                         private\n",
            "amenities                    [\"Fire extinguisher\", \"Smoke alarm\", \"Gas stov...\n",
            "review_scores_rating                                                       4.8\n",
            "review_scores_cleanliness                                                  4.7\n",
            "review_scores_checkin                                                      4.8\n",
            "review_scores_communication                                                4.9\n",
            "review_scores_location                                                     4.6\n",
            "review_scores_value                                                        4.7\n",
            "number_of_reviews_ltm                                                        3\n",
            "host_is_superhost                                                         True\n",
            "host_response_rate                                                         1.0\n",
            "host_acceptance_rate                                                      0.89\n",
            "host_identity_verified                                                    True\n",
            "instant_bookable                                                         False\n",
            "month                                                                       11\n",
            "price_bin                                                                    0\n",
            "DataLoaders created with pin_memory=True and num_workers=2.\n",
            "\n",
            "--- Starting Model Training (Hybrid Architecture) ---\n",
            "Performing Step 0 evaluation...\n",
            "Step 0 evaluation complete.\n",
            "\n",
            "Steps | Epoch | Train RW-MSLE | Val RW-MSLE | Train Attr RMSE | Val Attr RMSE | Patience | Elapsed Time\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "    0 |  0.00 |        2.7764 |      2.6963 |            1.01 |          1.00 |        0 | 00:00:09\n",
            "   10 |  0.16 |        1.5087 |      1.2651 |            4.59 |          9.08 |        0 | 00:00:21\n",
            "   20 |  0.32 |        1.4296 |      1.6772 |         4343.85 |       6029.62 |        1 | 00:00:32\n",
            "   30 |  0.48 |        3.0298 |      3.2462 |       318091.07 |     335721.46 |        2 | 00:00:43\n",
            "   40 |  0.63 |        3.9874 |      3.9649 |      2105049.38 |    2181718.28 |        3 | 00:00:53\n",
            "   50 |  0.79 |        4.1736 |      4.2779 |      4118208.72 |    4941870.04 |        4 | 00:01:04\n",
            "   60 |  0.95 |        4.2446 |      4.4244 |      7200002.58 |    7232145.59 |        5 | 00:01:15\n",
            "\n",
            "--- Early Stopping Triggered at Step 60 ---\n",
            "\n",
            "--- Training Complete ---\n",
            "Loading best model state (Val RW-MSLE: 1.2651)\n",
            "\n",
            "Saving artifacts to ./nyc_artifacts_20251009_155058.pt and /content/drive/MyDrive/Colab_Notebooks/Airbnb_Project/nyc_artifacts_20251009_155058.pt...\n",
            "Artifacts saved successfully.\n",
            "\n",
            "--- Final Training History ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x789004281190>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_a53da\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_a53da_level0_col0\" class=\"col_heading level0 col0\" >Epoch</th>\n",
              "      <th id=\"T_a53da_level0_col1\" class=\"col_heading level0 col1\" >Train RW-MSLE</th>\n",
              "      <th id=\"T_a53da_level0_col2\" class=\"col_heading level0 col2\" >Val RW-MSLE</th>\n",
              "      <th id=\"T_a53da_level0_col3\" class=\"col_heading level0 col3\" >Train Attr RMSE</th>\n",
              "      <th id=\"T_a53da_level0_col4\" class=\"col_heading level0 col4\" >Val Attr RMSE</th>\n",
              "      <th id=\"T_a53da_level0_col5\" class=\"col_heading level0 col5\" >Patience</th>\n",
              "      <th id=\"T_a53da_level0_col6\" class=\"col_heading level0 col6\" >Elapsed Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" >Steps</th>\n",
              "      <th class=\"blank col0\" >&nbsp;</th>\n",
              "      <th class=\"blank col1\" >&nbsp;</th>\n",
              "      <th class=\"blank col2\" >&nbsp;</th>\n",
              "      <th class=\"blank col3\" >&nbsp;</th>\n",
              "      <th class=\"blank col4\" >&nbsp;</th>\n",
              "      <th class=\"blank col5\" >&nbsp;</th>\n",
              "      <th class=\"blank col6\" >&nbsp;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_a53da_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_a53da_row0_col0\" class=\"data row0 col0\" >0.00</td>\n",
              "      <td id=\"T_a53da_row0_col1\" class=\"data row0 col1\" >2.7764</td>\n",
              "      <td id=\"T_a53da_row0_col2\" class=\"data row0 col2\" >2.6963</td>\n",
              "      <td id=\"T_a53da_row0_col3\" class=\"data row0 col3\" >1.01</td>\n",
              "      <td id=\"T_a53da_row0_col4\" class=\"data row0 col4\" >1.00</td>\n",
              "      <td id=\"T_a53da_row0_col5\" class=\"data row0 col5\" >0</td>\n",
              "      <td id=\"T_a53da_row0_col6\" class=\"data row0 col6\" >00:00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a53da_level0_row1\" class=\"row_heading level0 row1\" >10</th>\n",
              "      <td id=\"T_a53da_row1_col0\" class=\"data row1 col0\" >0.16</td>\n",
              "      <td id=\"T_a53da_row1_col1\" class=\"data row1 col1\" >1.5087</td>\n",
              "      <td id=\"T_a53da_row1_col2\" class=\"data row1 col2\" >1.2651</td>\n",
              "      <td id=\"T_a53da_row1_col3\" class=\"data row1 col3\" >4.59</td>\n",
              "      <td id=\"T_a53da_row1_col4\" class=\"data row1 col4\" >9.08</td>\n",
              "      <td id=\"T_a53da_row1_col5\" class=\"data row1 col5\" >0</td>\n",
              "      <td id=\"T_a53da_row1_col6\" class=\"data row1 col6\" >00:00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a53da_level0_row2\" class=\"row_heading level0 row2\" >20</th>\n",
              "      <td id=\"T_a53da_row2_col0\" class=\"data row2 col0\" >0.32</td>\n",
              "      <td id=\"T_a53da_row2_col1\" class=\"data row2 col1\" >1.4296</td>\n",
              "      <td id=\"T_a53da_row2_col2\" class=\"data row2 col2\" >1.6772</td>\n",
              "      <td id=\"T_a53da_row2_col3\" class=\"data row2 col3\" >4343.85</td>\n",
              "      <td id=\"T_a53da_row2_col4\" class=\"data row2 col4\" >6029.62</td>\n",
              "      <td id=\"T_a53da_row2_col5\" class=\"data row2 col5\" >1</td>\n",
              "      <td id=\"T_a53da_row2_col6\" class=\"data row2 col6\" >00:00:32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a53da_level0_row3\" class=\"row_heading level0 row3\" >30</th>\n",
              "      <td id=\"T_a53da_row3_col0\" class=\"data row3 col0\" >0.48</td>\n",
              "      <td id=\"T_a53da_row3_col1\" class=\"data row3 col1\" >3.0298</td>\n",
              "      <td id=\"T_a53da_row3_col2\" class=\"data row3 col2\" >3.2462</td>\n",
              "      <td id=\"T_a53da_row3_col3\" class=\"data row3 col3\" >318091.07</td>\n",
              "      <td id=\"T_a53da_row3_col4\" class=\"data row3 col4\" >335721.46</td>\n",
              "      <td id=\"T_a53da_row3_col5\" class=\"data row3 col5\" >2</td>\n",
              "      <td id=\"T_a53da_row3_col6\" class=\"data row3 col6\" >00:00:43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a53da_level0_row4\" class=\"row_heading level0 row4\" >40</th>\n",
              "      <td id=\"T_a53da_row4_col0\" class=\"data row4 col0\" >0.63</td>\n",
              "      <td id=\"T_a53da_row4_col1\" class=\"data row4 col1\" >3.9874</td>\n",
              "      <td id=\"T_a53da_row4_col2\" class=\"data row4 col2\" >3.9649</td>\n",
              "      <td id=\"T_a53da_row4_col3\" class=\"data row4 col3\" >2105049.38</td>\n",
              "      <td id=\"T_a53da_row4_col4\" class=\"data row4 col4\" >2181718.28</td>\n",
              "      <td id=\"T_a53da_row4_col5\" class=\"data row4 col5\" >3</td>\n",
              "      <td id=\"T_a53da_row4_col6\" class=\"data row4 col6\" >00:00:53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a53da_level0_row5\" class=\"row_heading level0 row5\" >50</th>\n",
              "      <td id=\"T_a53da_row5_col0\" class=\"data row5 col0\" >0.79</td>\n",
              "      <td id=\"T_a53da_row5_col1\" class=\"data row5 col1\" >4.1736</td>\n",
              "      <td id=\"T_a53da_row5_col2\" class=\"data row5 col2\" >4.2779</td>\n",
              "      <td id=\"T_a53da_row5_col3\" class=\"data row5 col3\" >4118208.72</td>\n",
              "      <td id=\"T_a53da_row5_col4\" class=\"data row5 col4\" >4941870.04</td>\n",
              "      <td id=\"T_a53da_row5_col5\" class=\"data row5 col5\" >4</td>\n",
              "      <td id=\"T_a53da_row5_col6\" class=\"data row5 col6\" >00:01:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a53da_level0_row6\" class=\"row_heading level0 row6\" >60</th>\n",
              "      <td id=\"T_a53da_row6_col0\" class=\"data row6 col0\" >0.95</td>\n",
              "      <td id=\"T_a53da_row6_col1\" class=\"data row6 col1\" >4.2446</td>\n",
              "      <td id=\"T_a53da_row6_col2\" class=\"data row6 col2\" >4.4244</td>\n",
              "      <td id=\"T_a53da_row6_col3\" class=\"data row6 col3\" >7200002.58</td>\n",
              "      <td id=\"T_a53da_row6_col4\" class=\"data row6 col4\" >7232145.59</td>\n",
              "      <td id=\"T_a53da_row6_col5\" class=\"data row6 col5\" >5</td>\n",
              "      <td id=\"T_a53da_row6_col6\" class=\"data row6 col6\" >00:01:15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def analyze_hybrid_model(model, data_loader, df, device):\n",
        "    \"\"\"\n",
        "    Analyzes the performance and explanations of the hybrid model for a given dataset.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Store all outputs from the model\n",
        "    q_preds, targets = [], []\n",
        "    contributions = { \"p_loc\": [], \"p_size\": [], \"p_qual\": [], \"p_amenities\": [], \"p_season\": [], \"p_bias\": [] }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            for key, value in batch.items():\n",
        "                if isinstance(value, torch.Tensor): batch[key] = value.to(device, non_blocking=True)\n",
        "                elif isinstance(value, dict):\n",
        "                    for sub_key, sub_value in value.items(): batch[key][sub_key] = sub_value.to(device, non_blocking=True)\n",
        "\n",
        "            preds = model(batch)\n",
        "\n",
        "            q_preds.extend(preds['q_pred'].cpu().numpy())\n",
        "            targets.extend(batch['target'].cpu().numpy())\n",
        "\n",
        "            for key in contributions.keys():\n",
        "                contributions[key].extend(preds[key].cpu().numpy())\n",
        "\n",
        "    # Create a detailed results dataframe\n",
        "    results_df = df.iloc[:len(q_preds)].copy()\n",
        "    results_df['target_price'] = targets\n",
        "    results_df['predicted_log_price'] = q_preds\n",
        "\n",
        "    # Use np.expm1 to correctly invert np.log1p\n",
        "    results_df['predicted_price'] = np.expm1(results_df['predicted_log_price'])\n",
        "\n",
        "    for key, value in contributions.items():\n",
        "        results_df[key] = value\n",
        "\n",
        "    results_df['p_sum'] = results_df[list(contributions.keys())].sum(axis=1)\n",
        "\n",
        "    # --- Perform Analyses ---\n",
        "\n",
        "    # 1. MAPE by Price Bracket\n",
        "    results_df['percentage_error'] = ((results_df['predicted_price'] - results_df['target_price']).abs() / results_df['target_price']) * 100\n",
        "    price_bins = [0, 75, 150, 250, 400, results_df['target_price'].max() + 1]\n",
        "    results_df['price_bin'] = pd.cut(results_df['target_price'], bins=price_bins, right=False)\n",
        "    performance_summary = results_df.groupby('price_bin').agg(mape=('percentage_error', 'mean')).reset_index()\n",
        "\n",
        "    return performance_summary, results_df"
      ],
      "metadata": {
        "id": "XgXZLdA6o5tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " --- Main Analysis Execution ---\n",
        "\n",
        "# 1. Unpack the artifacts from your training run\n",
        "print(\"Unpacking artifacts...\")\n",
        "processor = training_artifacts['processor']\n",
        "config = training_artifacts['config']\n",
        "model_state_dict = training_artifacts['model_state_dict']\n",
        "\n",
        "# 2. Recreate the data splits and data loaders\n",
        "print(\"Recreating data splits and data loaders...\")\n",
        "train_df, val_df = load_and_split_data(config)\n",
        "train_features_cpu = preprocess_and_tensorize_CPU(processor, train_df)\n",
        "val_features_cpu = preprocess_and_tensorize_CPU(processor, val_df)\n",
        "train_loader, val_loader = create_dataloaders(train_features_cpu, val_features_cpu, config)\n",
        "print(\"Data loaders recreated.\")\n",
        "\n",
        "# 3. Instantiate a new model and load the trained (best) weights\n",
        "print(\"Loading trained model...\")\n",
        "hybrid_model = HybridExplainableModel(processor, config.DEVICE)\n",
        "hybrid_model.load_state_dict(model_state_dict)\n",
        "print(\"Model loaded.\")\n",
        "\n",
        "# 4. Analyze performance on both TRAINING and VALIDATION sets\n",
        "print(\"\\n--- Analyzing performance on TRAINING set ---\")\n",
        "train_summary, train_results_df = analyze_hybrid_model(hybrid_model, train_loader, train_df, config.DEVICE)\n",
        "print(\"\\n--- Analyzing performance on VALIDATION set ---\")\n",
        "val_summary, val_results_df = analyze_hybrid_model(hybrid_model, val_loader, val_df, config.DEVICE)\n",
        "\n",
        "# --- 5. Print Key Statistics ---\n",
        "\n",
        "# Sanity Check\n",
        "train_consistency_error = (train_results_df['p_sum'] - train_results_df['predicted_price']).abs().mean()\n",
        "val_consistency_error = (val_results_df['p_sum'] - val_results_df['predicted_price']).abs().mean()\n",
        "print(\"\\n--- Sanity Check: Does (contributions)  predicted price? ---\")\n",
        "print(f\"Mean Absolute Difference (Train): ${train_consistency_error:.2f}\")\n",
        "print(f\"Mean Absolute Difference (Validation): ${val_consistency_error:.2f}\")\n",
        "\n",
        "# MAPE Summaries\n",
        "print(\"\\n--- MAPE by Price Bracket (Validation Set) ---\")\n",
        "print(val_summary.to_string(index=False, float_format=\"%.2f\"))\n",
        "\n",
        "# Average Contributions\n",
        "print(\"\\n--- Average Dollar Contributions (Validation Set) ---\")\n",
        "val_avg_contrib = val_results_df[['p_bias', 'p_loc', 'p_size', 'p_qual', 'p_amenities', 'p_season']].mean()\n",
        "print(val_avg_contrib)\n",
        "\n",
        "# --- 6. Generate Visualizations ---\n",
        "\n",
        "print(\"\\nGenerating plots...\")\n",
        "\n",
        "# Figure 1: MAPE by Price Bracket\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 7), sharey=True)\n",
        "fig.suptitle('Model Accuracy Analysis: Hybrid Model', fontsize=20)\n",
        "\n",
        "sns.barplot(ax=axes[0], data=train_summary, x='price_bin', y='mape', palette='plasma')\n",
        "axes[0].set_title('Training Set: MAPE by Price Bracket', fontsize=16)\n",
        "axes[0].set_xlabel('True Price Range ($)', fontsize=12)\n",
        "axes[0].set_ylabel('Mean Absolute Percentage Error (MAPE %)', fontsize=12)\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "axes[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "sns.barplot(ax=axes[1], data=val_summary, x='price_bin', y='mape', palette='plasma')\n",
        "axes[1].set_title('Validation Set: MAPE by Price Bracket', fontsize=16)\n",
        "axes[1].set_xlabel('True Price Range ($)', fontsize=12)\n",
        "axes[1].set_ylabel('')\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "axes[1].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "# Figure 2: Average Dollar Contributions\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 7), sharey=True)\n",
        "fig.suptitle('Model Explanation Analysis: Average Dollar Contributions', fontsize=20)\n",
        "\n",
        "train_avg_contrib = train_results_df[['p_bias', 'p_loc', 'p_size', 'p_qual', 'p_amenities', 'p_season']].mean()\n",
        "train_avg_contrib.plot(kind='bar', ax=axes[0], color=sns.color_palette('viridis', 6))\n",
        "axes[0].set_title('Training Set', fontsize=16)\n",
        "axes[0].set_ylabel('Average Contribution ($)', fontsize=12)\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "axes[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "val_avg_contrib.plot(kind='bar', ax=axes[1], color=sns.color_palette('viridis', 6))\n",
        "axes[1].set_title('Validation Set', fontsize=16)\n",
        "axes[1].set_ylabel('')\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "axes[1].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4psZZrWypueJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}