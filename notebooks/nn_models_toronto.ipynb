{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arvindsuresh-math/Fall-2025-Team-Big-Data/blob/main/final_notebooks/nn_models_toronto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d71cad3",
      "metadata": {
        "id": "6d71cad3"
      },
      "source": [
        "# 1. Baseline Neural Network Model\n",
        "\n",
        "### Objective\n",
        "\n",
        "In this notebook, we train and evaluate a fully-connected deep learning model. This model will serve as a robust performance baseline against which we can compare our more complex, interpretable `AdditiveModel`.\n",
        "\n",
        "The architecture is a standard Multi-Layer Perceptron (MLP) that takes all available features—including location, size, quality, and text embeddings—concatenates them into a single vector, and processes them through several layers to predict the final log-price deviation. Regularization techniques like Dropout, Batch Normalization, and Weight Decay are used to prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8fdff920",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fdff920",
        "outputId": "2da74149-426b-42ac-c71a-4f422057a02c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Current working directory: /content/drive/MyDrive/Airbnb_Price_Project\n"
          ]
        }
      ],
      "source": [
        "# --- Mount Google Drive ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Change Directory to Project Folder ---\n",
        "import os\n",
        "\n",
        "# IMPORTANT: Make sure this path matches the location of your project folder in Google Drive\n",
        "PROJECT_PATH = '/content/drive/MyDrive/Airbnb_Price_Project'\n",
        "os.chdir(PROJECT_PATH)\n",
        "print(f\"Current working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Hugging Face Authentication ---\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "print(\"\\nAttempting Hugging Face login...\")\n",
        "try:\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    login(token=HF_TOKEN)\n",
        "    print(\"Hugging Face login successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not log in. Please ensure 'HF_TOKEN' is a valid secret. Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6ZLgwn3RDQf",
        "outputId": "05690436-b6c2-402e-f2ac-579d2c2d3f42"
      },
      "id": "Y6ZLgwn3RDQf",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Attempting Hugging Face login...\n",
            "Hugging Face login successful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Installing required packages ---\")\n",
        "!pip install -q pandas pyarrow sentence-transformers scikit-learn torch tqdm transformers matplotlib seaborn\n",
        "\n",
        "print(\"Package installation complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LObqTjWREzL",
        "outputId": "f11729bb-850d-47f9-8ba0-e580a4d12a88"
      },
      "id": "7LObqTjWREzL",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Installing required packages ---\n",
            "Package installation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# --- Custom Project Scripts ---\n",
        "from config import config\n",
        "from data_processing import load_and_split_data, FeatureProcessor, create_dataloaders\n",
        "# Import BOTH model classes and the dataset\n",
        "from model import BaselineModel, AdditiveModel, AirbnbPriceDataset\n",
        "from train import train_model\n",
        "# Import BOTH inference functions\n",
        "from inference import run_inference\n",
        "from build_app_dataset import build_dataset, create_full_panel_dataset"
      ],
      "metadata": {
        "id": "e9DpwNxNVGqY"
      },
      "id": "e9DpwNxNVGqY",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed: int):\n",
        "    \"\"\"\n",
        "    Sets random seeds for numpy, torch, and Python's random module to ensure\n",
        "    reproducible results across runs.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        # These settings are needed for full determinism with CUDA\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "    print(f\"All random seeds set to {seed}.\")\n",
        "\n",
        "set_seed(config[\"SEED\"])\n",
        "print(\"-\"*60)\n",
        "print(\"Current configuration:\")\n",
        "for key, value in config.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ihqv5Ai4Rkqv",
        "outputId": "012159b8-bbee-47c9-9734-bea7419ba2ce"
      },
      "id": "Ihqv5Ai4Rkqv",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All random seeds set to 42.\n",
            "------------------------------------------------------------\n",
            "Current configuration:\n",
            "CITY: toronto\n",
            "DEVICE: cuda\n",
            "DRIVE_SAVE_PATH: /content/drive/MyDrive/Airbnb_Price_Project/artifacts/\n",
            "TEXT_MODEL_NAME: BAAI/bge-small-en-v1.5\n",
            "VAL_SIZE: 0.05\n",
            "SEED: 42\n",
            "BATCH_SIZE: 256\n",
            "VALIDATION_BATCH_SIZE: 512\n",
            "LEARNING_RATE: 0.001\n",
            "TRANSFORMER_LEARNING_RATE: 1e-05\n",
            "N_EPOCHS: 100\n",
            "WEIGHT_DECAY: 0.0001\n",
            "DROPOUT_RATE: 0.2\n",
            "GEO_EMBEDDING_DIM: 32\n",
            "HIDDEN_LAYERS_LOCATION: [32, 16]\n",
            "HIDDEN_LAYERS_SIZE_CAPACITY: [32, 16]\n",
            "HIDDEN_LAYERS_QUALITY: [32, 16]\n",
            "HIDDEN_LAYERS_AMENITIES: [64, 32]\n",
            "HIDDEN_LAYERS_DESCRIPTION: [64, 32]\n",
            "HIDDEN_LAYERS_SEASONALITY: [16]\n",
            "EARLY_STOPPING_PATIENCE: 3\n",
            "EARLY_STOPPING_MIN_DELTA: 0.0\n",
            "SCHEDULER_PATIENCE: 2\n",
            "SCHEDULER_FACTOR: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36de2fbf",
      "metadata": {
        "id": "36de2fbf"
      },
      "source": [
        "## Data Loading and Preprocessing\n",
        "\n",
        "We begin by loading the dataset and performing our custom stratified group split. This method ensures that all records for a single listing (`listing_id`) are confined to either the training or the validation set, which is crucial for preventing data leakage and obtaining a reliable performance estimate.\n",
        "\n",
        "Once split, we instantiate and `fit` our `FeatureProcessor` exclusively on the training data. This learns the necessary vocabularies and scaling parameters, which are then used to `transform` both the training and validation sets into numerical tensors ready for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "158b0a7a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "158b0a7a",
        "outputId": "356c593b-deb8-46f8-8c06-eb264a70d950"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data split: 82,065 train records, 4,327 validation records.\n",
            "\n",
            "Data pipeline complete. DataLoaders are ready for training.\n"
          ]
        }
      ],
      "source": [
        "# Load and split the data\n",
        "train_df, val_df, neighborhood_log_means, train_ids, val_ids = load_and_split_data(config)\n",
        "\n",
        "# Instantiate and fit the feature processor on the training data\n",
        "processor = FeatureProcessor(config)\n",
        "processor.fit(train_df)\n",
        "\n",
        "# Transform both datasets into feature dictionaries\n",
        "train_features = processor.transform(train_df, neighborhood_log_means)\n",
        "val_features = processor.transform(val_df, neighborhood_log_means)\n",
        "\n",
        "# Create the PyTorch DataLoaders\n",
        "train_loader, val_loader = create_dataloaders(train_features, val_features, config)\n",
        "\n",
        "print(\"\\nData pipeline complete. DataLoaders are ready for training.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36442d75",
      "metadata": {
        "id": "36442d75"
      },
      "source": [
        "---\n",
        "# Part 1: Baseline Model\n",
        "---\n",
        "\n",
        "First, we train the `BaselineModel`. This involves initializing the model and a standard optimizer, running the training loop, and saving all the necessary artifacts for later analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "68dab76a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68dab76a",
        "outputId": "73d839a0-2ee2-4caa-f2ec-08c4babb4e83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BaselineModel and its optimizer/scheduler have been initialized.\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the baseline model\n",
        "baseline_model = BaselineModel(processor, config)\n",
        "baseline_model.to(config['DEVICE'])\n",
        "\n",
        "# Instantiate the optimizer with weight decay for regularization\n",
        "baseline_optimizer = optim.AdamW(\n",
        "    baseline_model.parameters(),\n",
        "    lr=config['LEARNING_RATE'],\n",
        "    weight_decay=config['WEIGHT_DECAY']\n",
        ")\n",
        "\n",
        "# Instantiate the learning rate scheduler\n",
        "baseline_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    baseline_optimizer,\n",
        "    mode='min',\n",
        "    factor=config['SCHEDULER_FACTOR'],\n",
        "    patience=config['SCHEDULER_PATIENCE']\n",
        ")\n",
        "\n",
        "print(f\"BaselineModel and its optimizer/scheduler have been initialized.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "281a0a04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "281a0a04",
        "outputId": "ca654157-2825-4cfc-e4cb-11ea0fdbe50e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Training for BaselineModel on TORONTO ---\n",
            "Epoch |     Time |   Train RMSE | Train MAPE (%) |   Val RMSE | Val MAPE (%) | MAPE Gap (%) | Patience\n",
            "------------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    1 | 00:01:16 |       0.3695 |          31.44 |     0.3327 |        26.41 |        -5.03 |        0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    2 | 00:02:31 |       0.3195 |          26.48 |     0.3314 |        26.66 |         0.18 |        0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    3 | 00:03:45 |       0.3015 |          24.87 |     0.3318 |        28.18 |         3.31 |        0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    4 | 00:05:00 |       0.2876 |          23.61 |     0.3319 |        26.71 |         3.11 |        0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    5 | 00:06:15 |       0.2735 |          22.35 |     0.3302 |        26.76 |         4.41 |        1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    6 | 00:07:29 |       0.2661 |          21.66 |     0.3310 |        26.99 |         5.33 |        2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    7 | 00:08:43 |       0.2621 |          21.34 |     0.3296 |        27.23 |         5.89 |        3\n",
            "--- Early Stopping Triggered (MAPE Gap exceeded 4% for 3 epochs) ---\n",
            "\n",
            "--- Training Complete ---\n",
            "Loading best model state from file with Train MAPE: 23.61% (and MAPE Gap: 3.11%)\n"
          ]
        }
      ],
      "source": [
        "trained_baseline_model, baseline_history_df = train_model(\n",
        "    model=baseline_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=baseline_optimizer,\n",
        "    scheduler=baseline_scheduler,\n",
        "    config=config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Performance Metrics\n",
        "\n",
        "With the training complete, the `train_model` function has returned the model object with the weights from its best-performing epoch, defined by our custom criteria (lowest `train_mape` while the validation/train MAPE gap is under 4%).\n",
        "\n",
        "To get the definitive performance scores for our saved model, we now query the `history_df` using the **exact same logic** to identify the best epoch and extract its corresponding metrics."
      ],
      "metadata": {
        "id": "pMxszD4cs6WA"
      },
      "id": "pMxszD4cs6WA"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ff4211e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff4211e7",
        "outputId": "5e81a659-9f44-4125-ec64-6cedb3e5182a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "          Final Baseline Model Performance Metrics          \n",
            "(Extracted from Best Epoch: 4)\n",
            "============================================================\n",
            "Train RMSE:      0.2876\n",
            "Validation RMSE: 0.3319\n",
            "------------------------------------------------------------\n",
            "Train MAPE:      23.61%\n",
            "Validation MAPE: 26.71%\n",
            "MAPE Gap:        3.11%\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# 1. Filter the history to find all epochs that satisfy the gap constraint (< 4%)\n",
        "valid_epochs_df = baseline_history_df[baseline_history_df['mape_gap'] < 0.04]\n",
        "\n",
        "if not valid_epochs_df.empty:\n",
        "    # 2. From these valid epochs, find the index of the one with the lowest train MAPE\n",
        "    best_epoch_idx = valid_epochs_df['train_mape'].idxmin()\n",
        "\n",
        "    # 3. Select the entire row of metrics from that best epoch\n",
        "    best_metrics_series = baseline_history_df.loc[best_epoch_idx]\n",
        "\n",
        "    # 4. Convert the pandas Series to a dictionary\n",
        "    final_baseline_metrics = best_metrics_series.to_dict()\n",
        "\n",
        "    # --- Print a clear summary for verification ---\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"{'Final Baseline Model Performance Metrics':^60}\")\n",
        "    print(f\"(Extracted from Best Epoch: {int(final_baseline_metrics['epoch']) + 1})\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Train RMSE:      {final_baseline_metrics['train_rmse']:.4f}\")\n",
        "    print(f\"Validation RMSE: {final_baseline_metrics['val_rmse']:.4f}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"Train MAPE:      {final_baseline_metrics['train_mape'] * 100:.2f}%\")\n",
        "    print(f\"Validation MAPE: {final_baseline_metrics['val_mape'] * 100:.2f}%\")\n",
        "    print(f\"MAPE Gap:        {final_baseline_metrics['mape_gap'] * 100:.2f}%\")\n",
        "    print(\"=\" * 60)\n",
        "else:\n",
        "    print(\"ERROR: No valid epochs found that met the <4% MAPE gap criterion.\")\n",
        "    # Create a dummy dictionary to prevent the next cell from crashing\n",
        "    final_baseline_metrics = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "466e68d2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "466e68d2",
        "outputId": "de42c47d-9768-4669-f851-f36bd46311d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Preparing full panel dataset for baseline inference ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running Inference: 100%|██████████| 188/188 [04:26<00:00,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Saving all baseline artifacts ---\n",
            "Baseline artifacts for TORONTO successfully saved in folder: /content/drive/MyDrive/Airbnb_Price_Project/artifacts/toronto_baseline_20251105_164700\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Preparing full panel dataset for baseline inference ---\")\n",
        "raw_df = pd.read_parquet(f\"./{config['CITY']}_dataset_oct_20.parquet\")\n",
        "panel_df = create_full_panel_dataset(raw_df, train_ids, val_ids)\n",
        "panel_features = processor.transform(panel_df, neighborhood_log_means)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config['TEXT_MODEL_NAME'], use_fast=True)\n",
        "panel_dataset = AirbnbPriceDataset(panel_features, tokenizer)\n",
        "panel_loader = DataLoader(panel_dataset, batch_size=config['VALIDATION_BATCH_SIZE'], shuffle=False)\n",
        "predictions_df = run_inference(trained_baseline_model, panel_loader, config['DEVICE'])\n",
        "final_predictions_df = pd.concat([panel_df, predictions_df], axis=1)\n",
        "\n",
        "print(\"\\n--- Saving all baseline artifacts ---\")\n",
        "timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
        "# UPDATED: The directory name now includes the city\n",
        "artifacts_dir = os.path.join(config['DRIVE_SAVE_PATH'], f\"{config['CITY']}_baseline_{timestamp}\")\n",
        "os.makedirs(artifacts_dir, exist_ok=True)\n",
        "\n",
        "# UPDATED: All filenames are now prefixed with the city name\n",
        "model_save_path = os.path.join(artifacts_dir, f\"{config['CITY']}_baseline_model.pt\")\n",
        "processor_save_path = os.path.join(artifacts_dir, f\"{config['CITY']}_feature_processor.pkl\")\n",
        "predictions_save_path = os.path.join(artifacts_dir, f\"{config['CITY']}_baseline_model_predictions.parquet\")\n",
        "\n",
        "# Save model, processor, and predictions\n",
        "torch.save({\n",
        "    'model_state_dict': trained_baseline_model.state_dict(),\n",
        "    'final_metrics': final_baseline_metrics\n",
        "}, model_save_path)\n",
        "with open(processor_save_path, 'wb') as f:\n",
        "    pickle.dump(processor, f)\n",
        "final_predictions_df.to_parquet(predictions_save_path, index=False)\n",
        "\n",
        "print(f\"Baseline artifacts for {config['CITY'].upper()} successfully saved in folder: {artifacts_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93d49517",
      "metadata": {
        "id": "93d49517"
      },
      "source": [
        "---\n",
        "# Part 2: Additive Model\n",
        "---\n",
        "\n",
        "Now, we proceed to train the `AdditiveModel`. We reuse the exact same data loaders to ensure a fair comparison. The key difference in this section is the optimizer setup, which uses a lower learning rate for the pre-trained text transformer to enable effective fine-tuning. After training, we run the specialized `build_dataset` script to generate the enriched data artifact for our Streamlit application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2901253b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2901253b",
        "outputId": "de944f25-8ff6-4e1e-dd97-25727c201951"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdditiveModel and its optimizer/scheduler have been initialized.\n"
          ]
        }
      ],
      "source": [
        "additive_model = AdditiveModel(processor, config)\n",
        "additive_model.to(config['DEVICE'])\n",
        "\n",
        "# Create parameter groups for differential learning rates\n",
        "transformer_params = additive_model.text_transformer.parameters()\n",
        "other_params = [p for n, p in additive_model.named_parameters() if 'text_transformer' not in n]\n",
        "\n",
        "# Instantiate the optimizer with two parameter groups\n",
        "additive_optimizer = optim.AdamW([\n",
        "    {'params': other_params, 'lr': config['LEARNING_RATE'], 'weight_decay': config['WEIGHT_DECAY']},\n",
        "    {'params': transformer_params, 'lr': config['TRANSFORMER_LEARNING_RATE']}\n",
        "])\n",
        "\n",
        "additive_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    additive_optimizer, mode='min', factor=config['SCHEDULER_FACTOR'], patience=config['SCHEDULER_PATIENCE']\n",
        ")\n",
        "\n",
        "print(f\"AdditiveModel and its optimizer/scheduler have been initialized.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_additive_model, additive_history_df = train_model(\n",
        "    model=additive_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=additive_optimizer,\n",
        "    scheduler=additive_scheduler,\n",
        "    config=config\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPxT-a4Miqj5",
        "outputId": "4db364f4-fd33-4d22-bd7f-277c84e896af"
      },
      "id": "EPxT-a4Miqj5",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Training for AdditiveModel on TORONTO ---\n",
            "Epoch |     Time |   Train RMSE | Train MAPE (%) |   Val RMSE | Val MAPE (%) | MAPE Gap (%) | Patience\n",
            "------------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    1 | 00:01:12 |       0.4100 |          34.55 |     0.3478 |        29.84 |        -4.71 |        0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    2 | 00:02:25 |       0.3287 |          27.41 |     0.3449 |        29.37 |         1.96 |        0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    3 | 00:03:38 |       0.3100 |          25.74 |     0.3440 |        28.91 |         3.17 |        0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    4 | 00:04:50 |       0.2991 |          24.74 |     0.3441 |        29.40 |         4.67 |        1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    5 | 00:06:03 |       0.2917 |          24.01 |     0.3506 |        28.42 |         4.42 |        2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    6 | 00:07:15 |       0.2849 |          23.44 |     0.3490 |        29.88 |         6.44 |        3\n",
            "--- Early Stopping Triggered (MAPE Gap exceeded 4% for 3 epochs) ---\n",
            "\n",
            "--- Training Complete ---\n",
            "Loading best model state from file with Train MAPE: 25.74% (and MAPE Gap: 3.17%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Filter the history to find all epochs that satisfy the gap constraint (< 4%)\n",
        "valid_epochs_df = additive_history_df[additive_history_df['mape_gap'] < 0.04]\n",
        "\n",
        "if not valid_epochs_df.empty:\n",
        "    # 2. From these valid epochs, find the index of the one with the lowest train MAPE\n",
        "    best_epoch_idx = valid_epochs_df['train_mape'].idxmin()\n",
        "\n",
        "    # 3. Select the entire row of metrics from that best epoch\n",
        "    best_metrics_series = additive_history_df.loc[best_epoch_idx]\n",
        "\n",
        "    # 4. Convert the pandas Series to a dictionary\n",
        "    final_additive_metrics = best_metrics_series.to_dict()\n",
        "\n",
        "    # --- Print a clear summary for verification ---\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"{'Final Additive Model Performance Metrics':^60}\")\n",
        "    print(f\"(Extracted from Best Epoch: {int(final_additive_metrics['epoch']) + 1})\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Train RMSE:      {final_additive_metrics['train_rmse']:.4f}\")\n",
        "    print(f\"Validation RMSE: {final_additive_metrics['val_rmse']:.4f}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"Train MAPE:      {final_additive_metrics['train_mape'] * 100:.2f}%\")\n",
        "    print(f\"Validation MAPE: {final_additive_metrics['val_mape'] * 100:.2f}%\")\n",
        "    print(f\"MAPE Gap:        {final_additive_metrics['mape_gap'] * 100:.2f}%\")\n",
        "    print(\"=\" * 60)\n",
        "else:\n",
        "    print(\"ERROR: No valid epochs found for the Additive Model that met the <4% MAPE gap criterion.\")\n",
        "    # Create a dummy dictionary to prevent the next cell from crashing\n",
        "    final_additive_metrics = {}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRI0CKCkis9h",
        "outputId": "1fcf4503-2c11-4da8-f72e-6ce88b2d6a42"
      },
      "id": "TRI0CKCkis9h",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "          Final Additive Model Performance Metrics          \n",
            "(Extracted from Best Epoch: 3)\n",
            "============================================================\n",
            "Train RMSE:      0.3100\n",
            "Validation RMSE: 0.3440\n",
            "------------------------------------------------------------\n",
            "Train MAPE:      25.74%\n",
            "Validation MAPE: 28.91%\n",
            "MAPE Gap:        3.17%\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Building and saving final application dataset ---\")\n",
        "build_dataset(\n",
        "    model=trained_additive_model,\n",
        "    processor=processor,\n",
        "    config=config,\n",
        "    train_ids=train_ids,\n",
        "    val_ids=val_ids\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZdG1nCxuLAw",
        "outputId": "50a660dc-8ea7-4016-951d-274cb4803116"
      },
      "id": "tZdG1nCxuLAw",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Building and saving final application dataset ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running Detailed Inference: 100%|██████████| 188/188 [01:50<00:00,  1.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Successfully created application database at: /content/drive/MyDrive/Airbnb_Price_Project/artifacts/app_data/toronto_app_database.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We also save the core model artifacts for reproducibility.\n",
        "print(\"\\n--- Saving core additive model artifacts ---\")\n",
        "timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
        "# UPDATED: The directory name now includes the city\n",
        "artifacts_dir = os.path.join(config['DRIVE_SAVE_PATH'], f\"{config['CITY']}_additive_{timestamp}\")\n",
        "os.makedirs(artifacts_dir, exist_ok=True)\n",
        "\n",
        "# UPDATED: All filenames are now prefixed with the city name\n",
        "model_save_path = os.path.join(artifacts_dir, f\"{config['CITY']}_additive_model.pt\")\n",
        "processor_save_path = os.path.join(artifacts_dir, f\"{config['CITY']}_feature_processor.pkl\")\n",
        "\n",
        "# Save model and processor\n",
        "torch.save({\n",
        "    'model_state_dict': trained_additive_model.state_dict(),\n",
        "    'final_metrics': final_additive_metrics\n",
        "}, model_save_path)\n",
        "with open(processor_save_path, 'wb') as f:\n",
        "    pickle.dump(processor, f)\n",
        "\n",
        "print(f\"Core additive artifacts for {config['CITY'].upper()} successfully saved in folder: {artifacts_dir}\")\n",
        "print(\"\\n\\nNotebook complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILKipFx2kPgx",
        "outputId": "76958524-b007-4e09-f991-44e1e04a6d13"
      },
      "id": "ILKipFx2kPgx",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Saving core additive model artifacts ---\n",
            "Core additive artifacts for TORONTO successfully saved in folder: /content/drive/MyDrive/Airbnb_Price_Project/artifacts/toronto_additive_20251105_165611\n",
            "\n",
            "\n",
            "Notebook complete.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}