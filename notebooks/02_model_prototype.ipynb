{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d067fff9",
   "metadata": {},
   "source": [
    "### **Step 1: Setup, Data Loading, and Stratified Split**\n",
    "\n",
    "We will load the dataset as before. The key change is the splitting logic. We will create a new column by combining `neighbourhood_cleansed` and `month`. This ensures that every unique combination of neighborhood and month is proportionally represented in both the training and validation sets.\n",
    "\n",
    "A crucial edge case we must handle is strata with only one member. `train_test_split` cannot split a single sample, so we will identify and filter out these rare instances before performing the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2772af93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading final dataset from: /Users/arvindsuresh/Downloads/insideairbnb/nyc/nyc_final_modeling_dataset.parquet\n",
      "Dataset loaded successfully.\n",
      "\n",
      "Created a combined stratification key with 2203 unique strata.\n",
      "Original dataset size: 83,218\n",
      "Removed 230 records belonging to strata with only 1 member.\n",
      "Filtered dataset size for splitting: 82,988\n",
      "\n",
      "--- Data Split Summary ---\n",
      "Training records: 66,390\n",
      "Validation records: 16,598\n",
      "\n",
      "Month distribution in Training Set:\n",
      "month\n",
      "1     0.078461\n",
      "2     0.074695\n",
      "3     0.067842\n",
      "4     0.067826\n",
      "5     0.074108\n",
      "6     0.079425\n",
      "7     0.079485\n",
      "8     0.067601\n",
      "10    0.159256\n",
      "11    0.161380\n",
      "12    0.089923\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Month distribution in Validation Set:\n",
      "month\n",
      "1     0.078202\n",
      "2     0.074407\n",
      "3     0.068201\n",
      "4     0.067659\n",
      "5     0.074105\n",
      "6     0.079528\n",
      "7     0.079467\n",
      "8     0.066996\n",
      "10    0.159658\n",
      "11    0.161827\n",
      "12    0.089951\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Configuration ---\n",
    "CITY = \"nyc\"\n",
    "DATA_DIR = os.path.expanduser(f\"~/Downloads/insideairbnb/{CITY}\")\n",
    "DATASET_PATH = os.path.join(DATA_DIR, f\"{CITY}_final_modeling_dataset.parquet\")\n",
    "VAL_SIZE = 0.2  # Using 20% of the data for validation\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- Load Data ---\n",
    "print(f\"Loading final dataset from: {DATASET_PATH}\")\n",
    "df = pd.read_parquet(DATASET_PATH)\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "# --- Stratified Train/Validation Split ---\n",
    "# To stratify by both neighborhood and month, we create a combined key.\n",
    "stratify_col = df['neighbourhood_cleansed'].astype(str) + '_' + df['month'].astype(str)\n",
    "print(f\"\\nCreated a combined stratification key with {stratify_col.nunique()} unique strata.\")\n",
    "\n",
    "# -- Handle small strata --\n",
    "# train_test_split requires at least 2 members per stratum. We identify and\n",
    "# filter out strata with only one sample.\n",
    "strata_counts = stratify_col.value_counts()\n",
    "valid_strata = strata_counts[strata_counts >= 2].index\n",
    "df_filtered = df[stratify_col.isin(valid_strata)].copy()\n",
    "\n",
    "print(f\"Original dataset size: {len(df):,}\")\n",
    "print(f\"Removed {len(df) - len(df_filtered):,} records belonging to strata with only 1 member.\")\n",
    "print(f\"Filtered dataset size for splitting: {len(df_filtered):,}\")\n",
    "\n",
    "# Perform the stratified split on the filtered dataset\n",
    "# We split the indices first to avoid copying data in memory unnecessarily\n",
    "train_indices, val_indices = train_test_split(\n",
    "    df_filtered.index,\n",
    "    test_size=VAL_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=df_filtered['neighbourhood_cleansed'].astype(str) + '_' + df_filtered['month'].astype(str)\n",
    ")\n",
    "\n",
    "train_df = df_filtered.loc[train_indices].copy()\n",
    "val_df = df_filtered.loc[val_indices].copy()\n",
    "\n",
    "# Reset index for clean processing later\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "val_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"\\n--- Data Split Summary ---\")\n",
    "print(f\"Training records: {len(train_df):,}\")\n",
    "print(f\"Validation records: {len(val_df):,}\")\n",
    "\n",
    "# --- Verification ---\n",
    "# Check the distribution of 'month' in each set to verify stratification\n",
    "print(\"\\nMonth distribution in Training Set:\")\n",
    "print(train_df['month'].value_counts(normalize=True).sort_index())\n",
    "print(\"\\nMonth distribution in Validation Set:\")\n",
    "print(val_df['month'].value_counts(normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf4d861",
   "metadata": {},
   "source": [
    "### **Step 2: The `FeatureProcessor` Class**\n",
    "\n",
    "This is a critical stage where we translate the feature representation strategy outlined in `EMBEDDINGS.md` into code. The `FeatureProcessor` will be a reusable class, similar to a `scikit-learn` transformer. It will first learn all necessary transformations from the training data (`fit` method) and then apply these transformations to any dataset (`transform` method).\n",
    "\n",
    "This approach ensures two things:\n",
    "1.  **No Data Leakage:** Information from the validation set (like its mean or unique categories) never influences the transformations.\n",
    "2.  **Consistency:** The exact same transformations are applied to both training and validation data, which is essential for the model to work correctly.\n",
    "\n",
    "The processor will handle all transformations except for the `amenities` embedding, which will be done inside the PyTorch model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29e78604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class FeatureProcessor:\n",
    "    \"\"\"\n",
    "    A class to handle all feature transformations for the Airbnb pricing model.\n",
    "    It learns transformations from the training data and applies them consistently.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim_geo: int = 32): # Increased default geo dim\n",
    "        self.vocabs = {}\n",
    "        self.scalers = {}\n",
    "        self.embedding_dim_geo = embedding_dim_geo\n",
    "        \n",
    "        # --- Define feature groups based on EMBEDDINGS.md ---\n",
    "        self.categorical_cols = [\n",
    "            \"neighbourhood_cleansed\", \"property_type\", \"room_type\", \n",
    "            \"bathrooms_type\", \"bedrooms\", \"beds\", \"bathrooms_numeric\"\n",
    "        ]\n",
    "        self.numerical_cols = [\n",
    "            \"accommodates\", \"review_scores_rating\", \"review_scores_cleanliness\",\n",
    "            \"review_scores_checkin\", \"review_scores_communication\", \n",
    "            \"review_scores_location\", \"review_scores_value\", \"host_response_rate\",\n",
    "            \"host_acceptance_rate\"\n",
    "        ]\n",
    "        self.log_transform_cols = [\"number_of_reviews_ltm\"]\n",
    "        self.boolean_cols = [\n",
    "            \"host_is_superhost\", \"host_identity_verified\", \"instant_bookable\"\n",
    "        ]\n",
    "\n",
    "    def _create_positional_encoding(self, value, max_val):\n",
    "        \"\"\"Creates a positional encoding for a scalar value.\"\"\"\n",
    "        d = self.embedding_dim_geo\n",
    "        if d % 2 != 0:\n",
    "            raise ValueError(\"embedding_dim_geo must be an even number.\")\n",
    "            \n",
    "        pe = np.zeros(d)\n",
    "        position = (value / max_val) * 10000  # Scale position\n",
    "        div_term = np.exp(np.arange(0, d, 2) * -(np.log(10000.0) / d))\n",
    "        pe[0::2] = np.sin(position * div_term)\n",
    "        pe[1::2] = np.cos(position * div_term)\n",
    "        return pe\n",
    "\n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        \"\"\"Learns vocabularies and scaling parameters from the training data.\"\"\"\n",
    "        print(\"Fitting FeatureProcessor on training data...\")\n",
    "        \n",
    "        # 1. Learn vocabularies for categorical features\n",
    "        for col in self.categorical_cols:\n",
    "            # --- FIX IS HERE ---\n",
    "            # Drop NA values before getting unique elements to prevent sorting errors\n",
    "            valid_uniques = df[col].dropna().unique().tolist()\n",
    "            unique_vals = [\"<UNK>\"] + sorted(valid_uniques)\n",
    "            self.vocabs[col] = {val: i for i, val in enumerate(unique_vals)}\n",
    "            \n",
    "        # 2. Learn scaling parameters for numerical features\n",
    "        for col in self.numerical_cols + self.log_transform_cols:\n",
    "            if col in self.log_transform_cols:\n",
    "                vals = np.log1p(df[col])\n",
    "            else:\n",
    "                vals = df[col]\n",
    "            self.scalers[col] = {'mean': vals.mean(), 'std': vals.std()}\n",
    "        \n",
    "        print(\"Fit complete. Vocabularies and scalers have been learned.\")\n",
    "\n",
    "    def transform(self, df: pd.DataFrame) -> dict:\n",
    "        \"\"\"Applies learned transformations to a dataframe.\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # --- AXIS 1: Location ---\n",
    "        df[\"neighbourhood_cleansed_idx\"] = df[\"neighbourhood_cleansed\"].apply(\n",
    "            lambda x: self.vocabs[\"neighbourhood_cleansed\"].get(x, 0) # 0 is '<UNK>'\n",
    "        )\n",
    "        # Per EMBEDDINGS.md, Positional Encoding for lat/lon has output dim 32\n",
    "        lat_enc = df['latitude'].apply(lambda x: self._create_positional_encoding(x, 90))\n",
    "        lon_enc = df['longitude'].apply(lambda x: self._create_positional_encoding(x, 180))\n",
    "        # This results in two 16-dim vectors, matching the original 32-dim plan.\n",
    "        # Let's adjust _create_positional_encoding to just produce one vector of size 16 each\n",
    "        # A simpler way is to just halve the embedding_dim_geo inside the PE function\n",
    "        # For simplicity, let's keep the PE dim 32 and just concat them. No, let's follow the doc.\n",
    "        # Doc says \"Positional (Cyclical) Encoding | 32\". Let's assume that's for the combined lat/lon.\n",
    "        # A common way is 16 for lat, 16 for lon.\n",
    "        if self.embedding_dim_geo % 2 != 0:\n",
    "            raise ValueError(\"embedding_dim_geo must be an even number.\")\n",
    "        half_dim = self.embedding_dim_geo // 2\n",
    "        lat_enc = df['latitude'].apply(lambda x: self._create_positional_encoding(x, 90)[:half_dim])\n",
    "        lon_enc = df['longitude'].apply(lambda x: self._create_positional_encoding(x, 180)[:half_dim])\n",
    "        loc_geo = np.hstack([np.stack(lat_enc), np.stack(lon_enc)])\n",
    "\n",
    "        location_features = {\n",
    "            \"geo_position\": loc_geo,\n",
    "            \"neighbourhood\": df[\"neighbourhood_cleansed_idx\"].values\n",
    "        }\n",
    "        \n",
    "        # --- AXIS 2: Size & Capacity ---\n",
    "        size_capacity_features = {}\n",
    "        for col in [\"property_type\", \"room_type\", \"bathrooms_type\", \"bedrooms\", \"beds\", \"bathrooms_numeric\"]:\n",
    "            # Handle potential NaNs by mapping them to the '<UNK>' token\n",
    "            df[f\"{col}_idx\"] = df[col].apply(lambda x: self.vocabs[col].get(x, 0) if pd.notna(x) else 0)\n",
    "            size_capacity_features[col] = df[f\"{col}_idx\"].values\n",
    "        df[\"accommodates_std\"] = (df[\"accommodates\"] - self.scalers[\"accommodates\"][\"mean\"]) / self.scalers[\"accommodates\"][\"std\"]\n",
    "        size_capacity_features[\"accommodates\"] = df[\"accommodates_std\"].values\n",
    "\n",
    "        # --- AXIS 3: Quality & Reputation ---\n",
    "        quality_features = {}\n",
    "        numerical_quality_cols = [c for c in self.numerical_cols if c != \"accommodates\"]\n",
    "        for col in numerical_quality_cols:\n",
    "            df[f\"{col}_std\"] = (df[col] - self.scalers[col][\"mean\"]) / self.scalers[col][\"std\"]\n",
    "            quality_features[col] = df[f\"{col}_std\"].values\n",
    "        df[\"number_of_reviews_ltm_log_std\"] = (np.log1p(df[\"number_of_reviews_ltm\"]) - self.scalers[\"number_of_reviews_ltm\"][\"mean\"]) / self.scalers[\"number_of_reviews_ltm\"][\"std\"]\n",
    "        quality_features[\"number_of_reviews_ltm\"] = df[\"number_of_reviews_ltm_log_std\"].values\n",
    "        # One-hot encoding for booleans\n",
    "        for col in self.boolean_cols:\n",
    "             quality_features[col] = df[col].astype(float).values\n",
    "\n",
    "        # --- AXIS 4: Amenities ---\n",
    "        amenities_features = {\"text\": df[\"amenities\"].tolist()}\n",
    "        \n",
    "        # --- AXIS 5: Seasonality ---\n",
    "        month_sin = np.sin(2 * np.pi * df[\"month\"] / 12)\n",
    "        month_cos = np.cos(2 * np.pi * df[\"month\"] / 12)\n",
    "        seasonality_features = {\"cyclical\": np.vstack([month_sin, month_cos]).T}\n",
    "        \n",
    "        # --- Return final dictionary ---\n",
    "        return {\n",
    "            \"location\": location_features,\n",
    "            \"size_capacity\": size_capacity_features,\n",
    "            \"quality\": quality_features,\n",
    "            \"amenities\": amenities_features,\n",
    "            \"seasonality\": seasonality_features,\n",
    "            \"target_price\": df[\"target_price\"].values,\n",
    "            \"sample_weight\": df[\"estimated_occupancy_rate\"].values\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d48b8d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting FeatureProcessor on training data...\n",
      "Fit complete. Vocabularies and scalers have been learned.\n",
      "\n",
      "--- Transformation Output Verification ---\n",
      "Transformed data is a dictionary with keys: ['location', 'size_capacity', 'quality', 'amenities', 'seasonality', 'target_price', 'sample_weight']\n",
      "Sample from 'location' axis features:\n",
      "Geo Position Shape: (66390, 32)\n"
     ]
    }
   ],
   "source": [
    "# --- Code to run the processor ---\n",
    "# (You can run this in a separate cell)\n",
    "\n",
    "processor = FeatureProcessor()\n",
    "processor.fit(train_df)\n",
    "train_features = processor.transform(train_df)\n",
    "val_features = processor.transform(val_df)\n",
    "print(\"\\n--- Transformation Output Verification ---\")\n",
    "print(\"Transformed data is a dictionary with keys:\", list(train_features.keys()))\n",
    "print(\"Sample from 'location' axis features:\")\n",
    "print(\"Geo Position Shape:\", train_features['location']['geo_position'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f6d17b",
   "metadata": {},
   "source": [
    "### **Step 3: PyTorch `Dataset` and `DataLoader`**\n",
    "\n",
    "Now that our features are processed into NumPy arrays, we need a standard way to feed them into a PyTorch model. This is accomplished in two stages:\n",
    "\n",
    "1.  **`Dataset`:** A custom class that organizes our features and tells PyTorch how to retrieve a single data point (`__getitem__`). This is where the just-in-time tokenization of the `amenities` text will occur.\n",
    "2.  **`DataLoader`:** A PyTorch utility that wraps our `Dataset` and efficiently serves up batches of data, handling shuffling and collation automatically.\n",
    "\n",
    "This approach is memory-efficient and is the standard for all PyTorch projects. Our design is informed by the specifications in `EMBEDDINGS.md` regarding the use of a pre-trained transformer for amenities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a45c1624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple MPS device found. Using MPS for acceleration.\n",
      "Using device: mps\n",
      "\n",
      "--- DataLoader Verification ---\n",
      "Number of training batches: 260\n",
      "Number of validation batches: 65\n",
      "\n",
      "Sample batch loaded. Verifying contents...\n",
      "Batch keys: ['loc_geo_position', 'loc_neighbourhood', 'size_property_type', 'size_room_type', 'size_bathrooms_type', 'size_bedrooms', 'size_beds', 'size_bathrooms_numeric', 'size_accommodates', 'qual_review_scores_rating', 'qual_review_scores_cleanliness', 'qual_review_scores_checkin', 'qual_review_scores_communication', 'qual_review_scores_location', 'qual_review_scores_value', 'qual_host_response_rate', 'qual_host_acceptance_rate', 'qual_number_of_reviews_ltm', 'qual_host_is_superhost', 'qual_host_identity_verified', 'qual_instant_bookable', 'season_cyclical', 'target_price', 'sample_weight', 'amenities_tokens']\n",
      "\n",
      "Shape of 'loc_geo_position': torch.Size([256, 32])\n",
      "Shape of 'size_property_type': torch.Size([256])\n",
      "Shape of 'amenities_tokens.input_ids': torch.Size([256, 128])\n",
      "Shape of 'target_price': torch.Size([256])\n",
      "Data type of 'loc_neighbourhood': torch.int64\n",
      "Data type of 'target_price': torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --- Configuration ---\n",
    "# As per EMBEDDINGS.md, we use a specific pre-trained model for amenities.\n",
    "TOKENIZER_MODEL = 'BAAI/bge-small-en-v1.5'  # Corrected model name\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Device selection\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "    print(\"Apple MPS device found. Using MPS for acceleration.\")\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- PyTorch Dataset Class ---\n",
    "\n",
    "class AirbnbPriceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset to handle the transformed Airbnb feature dictionary.\n",
    "    \"\"\"\n",
    "    def __init__(self, features: dict):\n",
    "        self.features = features\n",
    "        self.n_samples = len(features['target_price'])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, index: int) -> dict:\n",
    "        \"\"\"\n",
    "        Retrieves all features for a single sample and converts them to Tensors.\n",
    "        The raw amenities text is also returned for batch tokenization.\n",
    "        \"\"\"\n",
    "        item = {}\n",
    "        # --- Get features for each axis ---\n",
    "        # Note: We keep features as separate dictionary entries for clarity in the model's forward pass.\n",
    "        \n",
    "        # Location\n",
    "        item['loc_geo_position'] = torch.tensor(self.features['location']['geo_position'][index], dtype=torch.float32)\n",
    "        item['loc_neighbourhood'] = torch.tensor(self.features['location']['neighbourhood'][index], dtype=torch.long)\n",
    "        \n",
    "        # Size & Capacity\n",
    "        for key, val in self.features['size_capacity'].items():\n",
    "            item[f'size_{key}'] = torch.tensor(val[index], dtype=torch.float32 if key == 'accommodates' else torch.long)\n",
    "            \n",
    "        # Quality & Reputation\n",
    "        for key, val in self.features['quality'].items():\n",
    "            item[f'qual_{key}'] = torch.tensor(val[index], dtype=torch.float32)\n",
    "\n",
    "        # Amenities (pass raw text)\n",
    "        item['amenities_text'] = self.features['amenities']['text'][index]\n",
    "\n",
    "        # Seasonality\n",
    "        item['season_cyclical'] = torch.tensor(self.features['seasonality']['cyclical'][index], dtype=torch.float32)\n",
    "        \n",
    "        # --- Get labels and weights ---\n",
    "        item['target_price'] = torch.tensor(self.features['target_price'][index], dtype=torch.float32)\n",
    "        item['sample_weight'] = torch.tensor(self.features['sample_weight'][index], dtype=torch.float32)\n",
    "        \n",
    "        return item\n",
    "\n",
    "# --- Custom Collate Function for Batch Tokenization ---\n",
    "\n",
    "# Instantiate the tokenizer model once\n",
    "tokenizer_model = SentenceTransformer(TOKENIZER_MODEL, device=DEVICE)\n",
    "\n",
    "def custom_collate_fn(batch: list) -> dict:\n",
    "    \"\"\"\n",
    "    Custom collate function to handle batching and on-the-fly tokenization.\n",
    "    \"\"\"\n",
    "    collated_batch = {}\n",
    "    \n",
    "    # 1. Separate raw text from other features\n",
    "    amenities_texts = [item.pop('amenities_text') for item in batch]\n",
    "    \n",
    "    # 2. Use default collate for all other tensor features\n",
    "    first_item = batch[0]\n",
    "    for key in first_item.keys():\n",
    "        collated_batch[key] = torch.stack([d[key] for d in batch])\n",
    "        \n",
    "    # 3. Tokenize the batch of amenities texts\n",
    "    # The tokenizer handles padding and creates 'input_ids' and 'attention_mask'\n",
    "    tokenized_amenities = tokenizer_model.tokenizer(\n",
    "        amenities_texts, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        return_tensors='pt',\n",
    "        max_length=128 # A reasonable max length for amenities lists\n",
    "    )\n",
    "    collated_batch['amenities_tokens'] = tokenized_amenities\n",
    "    \n",
    "    return collated_batch\n",
    "\n",
    "# --- Instantiate Datasets and DataLoaders ---\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = AirbnbPriceDataset(train_features)\n",
    "val_dataset = AirbnbPriceDataset(val_features)\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True, # Shuffle training data\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, # No need to shuffle validation data\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "\n",
    "print(\"\\n--- DataLoader Verification ---\")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "\n",
    "# --- Inspect a single batch to verify shapes and types ---\n",
    "try:\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    print(\"\\nSample batch loaded. Verifying contents...\")\n",
    "    print(\"Batch keys:\", list(sample_batch.keys()))\n",
    "    \n",
    "    print(\"\\nShape of 'loc_geo_position':\", sample_batch['loc_geo_position'].shape)\n",
    "    print(\"Shape of 'size_property_type':\", sample_batch['size_property_type'].shape)\n",
    "    print(\"Shape of 'amenities_tokens.input_ids':\", sample_batch['amenities_tokens']['input_ids'].shape)\n",
    "    print(\"Shape of 'target_price':\", sample_batch['target_price'].shape)\n",
    "    print(\"Data type of 'loc_neighbourhood':\", sample_batch['loc_neighbourhood'].dtype) # Should be torch.long\n",
    "    print(\"Data type of 'target_price':\", sample_batch['target_price'].dtype)     # Should be torch.float32\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while inspecting the DataLoader batch: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ebe173",
   "metadata": {},
   "source": [
    "### **Step 4: The Additive Model Architecture (`nn.Module`)**\n",
    "\n",
    "We will now define the neural network. The architecture is composed of several distinct parts that mirror the feature axes:\n",
    "\n",
    "1.  **Embedding Layers:** For all categorical features.\n",
    "2.  **Pre-trained Transformer:** For the `amenities` text. We will use the `SentenceTransformer` model directly and freeze its weights for this baseline version.\n",
    "3.  **Sub-Networks (MLPs):** Five small, independent Multi-Layer Perceptrons (MLPs), one for each axis. Each MLP takes the processed features for its axis and outputs a single scalar value representing that axis's price contribution.\n",
    "4.  **Aggregation:** A final summation of the outputs from the five sub-networks plus a single, learnable `Global_Bias` term.\n",
    "\n",
    "This structure allows us to inspect the output of each sub-network individually, providing the desired explainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "456a4e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Verification ---\n",
      "Model instantiated successfully on device: cpu\n",
      "Output shape of a sample forward pass: torch.Size([256])\n",
      "Sample prediction value: -0.58\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class AdditiveAxisModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the 5-axis additive model for Airbnb price prediction.\n",
    "    - Architecture is based on MODELING.md (Phase 1: Simple Additive Baseline).\n",
    "    - Feature embedding strategy is based on EMBEDDINGS.md.\n",
    "    \"\"\"\n",
    "    def __init__(self, processor: FeatureProcessor, device: str = 'cpu'):\n",
    "        super().__init__()\n",
    "        self.vocabs = processor.vocabs\n",
    "        self.device = device\n",
    "        \n",
    "        # --- 1. Embedding Layers ---\n",
    "        # Based on output dimensions in EMBEDDINGS.md\n",
    "        self.embed_neighbourhood = nn.Embedding(len(self.vocabs['neighbourhood_cleansed']), 16)\n",
    "        self.embed_property_type = nn.Embedding(len(self.vocabs['property_type']), 8)\n",
    "        self.embed_room_type = nn.Embedding(len(self.vocabs['room_type']), 4)\n",
    "        self.embed_bathrooms_type = nn.Embedding(len(self.vocabs['bathrooms_type']), 2)\n",
    "        \n",
    "        # For ordinal features treated as categorical\n",
    "        self.embed_bedrooms = nn.Embedding(len(self.vocabs['bedrooms']), 4)\n",
    "        self.embed_beds = nn.Embedding(len(self.vocabs['beds']), 4)\n",
    "        self.embed_bathrooms_numeric = nn.Embedding(len(self.vocabs['bathrooms_numeric']), 4)\n",
    "        \n",
    "        # --- 2. Pre-trained Amenities Model ---\n",
    "        self.amenities_transformer = SentenceTransformer('BAAI/bge-small-en-v1.5')\n",
    "        # FREEZE the transformer weights for the baseline model\n",
    "        for param in self.amenities_transformer.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # --- 3. Sub-Network MLPs ---\n",
    "        # Input dimensions are calculated from the concatenated features for each axis.\n",
    "        # See EMBEDDINGS.md \"Final Sub-Network Inputs\" section for these values.\n",
    "        \n",
    "        # Location: 32 (Geo) + 16 (Neighbourhood) = 48\n",
    "        self.loc_subnet = nn.Sequential(\n",
    "            nn.Linear(48, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "        # Size & Capacity: 8+4+2+4+4+4 (Embeds) + 1 (Accommodates) = 27\n",
    "        self.size_subnet = nn.Sequential(\n",
    "            nn.Linear(27, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "        # Quality: 8 (Numericals) + 1 (Reviews LTM) + 3 (Booleans) = 12\n",
    "        self.qual_subnet = nn.Sequential(\n",
    "            nn.Linear(12, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "        # Amenities: 384 (Transformer Output Dim for bge-small)\n",
    "        self.amenities_subnet = nn.Linear(384, 1)\n",
    "        \n",
    "        # Seasonality: 2 (Cyclical)\n",
    "        self.season_subnet = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "        \n",
    "        # --- 4. Global Bias Term ---\n",
    "        self.global_bias = nn.Parameter(torch.randn(1))\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, batch: dict) -> torch.Tensor:\n",
    "        # Move all tensors in the batch to the correct device\n",
    "        for key, value in batch.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                batch[key] = value.to(self.device)\n",
    "            elif isinstance(value, dict): # For the amenities tokenizer output\n",
    "                 for sub_key, sub_value in value.items():\n",
    "                    batch[key][sub_key] = sub_value.to(self.device)\n",
    "\n",
    "        # --- Process Each Axis ---\n",
    "\n",
    "        # 1. Location Axis\n",
    "        loc_geo = batch['loc_geo_position']\n",
    "        loc_hood_embed = self.embed_neighbourhood(batch['loc_neighbourhood'])\n",
    "        loc_input = torch.cat([loc_geo, loc_hood_embed], dim=1)\n",
    "        p_location = self.loc_subnet(loc_input)\n",
    "\n",
    "        # 2. Size & Capacity Axis\n",
    "        size_prop_embed = self.embed_property_type(batch['size_property_type'])\n",
    "        size_room_embed = self.embed_room_type(batch['size_room_type'])\n",
    "        size_bath_type_embed = self.embed_bathrooms_type(batch['size_bathrooms_type'])\n",
    "        size_beds_embed = self.embed_beds(batch['size_beds'])\n",
    "        size_bedrooms_embed = self.embed_bedrooms(batch['size_bedrooms'])\n",
    "        size_bath_num_embed = self.embed_bathrooms_numeric(batch['size_bathrooms_numeric'])\n",
    "        size_accommodates = batch['size_accommodates'].unsqueeze(1)\n",
    "        \n",
    "        size_input = torch.cat([\n",
    "            size_prop_embed, size_room_embed, size_bath_type_embed,\n",
    "            size_beds_embed, size_bedrooms_embed, size_bath_num_embed,\n",
    "            size_accommodates\n",
    "        ], dim=1)\n",
    "        p_size = self.size_subnet(size_input)\n",
    "\n",
    "        # 3. Quality & Reputation Axis\n",
    "        qual_input = torch.cat([\n",
    "            batch['qual_review_scores_rating'].unsqueeze(1),\n",
    "            batch['qual_review_scores_cleanliness'].unsqueeze(1),\n",
    "            batch['qual_review_scores_checkin'].unsqueeze(1),\n",
    "            batch['qual_review_scores_communication'].unsqueeze(1),\n",
    "            batch['qual_review_scores_location'].unsqueeze(1),\n",
    "            batch['qual_review_scores_value'].unsqueeze(1),\n",
    "            batch['qual_host_response_rate'].unsqueeze(1),\n",
    "            batch['qual_host_acceptance_rate'].unsqueeze(1),\n",
    "            batch['qual_number_of_reviews_ltm'].unsqueeze(1),\n",
    "            batch['qual_host_is_superhost'].unsqueeze(1),\n",
    "            batch['qual_host_identity_verified'].unsqueeze(1),\n",
    "            batch['qual_instant_bookable'].unsqueeze(1)\n",
    "        ], dim=1)\n",
    "        p_quality = self.qual_subnet(qual_input)\n",
    "\n",
    "        # 4. Amenities Axis\n",
    "        # The sentence_transformers library expects a dictionary for the forward pass\n",
    "        amenities_tokens = batch['amenities_tokens']\n",
    "        amenities_output = self.amenities_transformer(amenities_tokens)\n",
    "        amenities_embed = amenities_output['sentence_embedding']\n",
    "        p_amenities = self.amenities_subnet(amenities_embed)\n",
    "\n",
    "        # 5. Seasonality Axis\n",
    "        season_input = batch['season_cyclical']\n",
    "        p_seasonality = self.season_subnet(season_input)\n",
    "\n",
    "        # --- Final Aggregation ---\n",
    "        predicted_price = (\n",
    "            self.global_bias +\n",
    "            p_location +\n",
    "            p_size +\n",
    "            p_quality +\n",
    "            p_amenities +\n",
    "            p_seasonality\n",
    "        ).squeeze(-1) # Squeeze to remove the trailing dimension of size 1\n",
    "\n",
    "        return predicted_price\n",
    "\n",
    "# --- Instantiate the model and verify ---\n",
    "# We pass the fitted processor to give the model access to vocab sizes\n",
    "model = AdditiveAxisModel(processor, device='cpu')\n",
    "\n",
    "# Perform a forward pass with the sample batch to ensure all dimensions are correct\n",
    "try:\n",
    "    with torch.no_grad(): # No need to calculate gradients for this test\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        sample_output = model(sample_batch)\n",
    "    \n",
    "    print(\"--- Model Verification ---\")\n",
    "    print(f\"Model instantiated successfully on device: {model.device}\")\n",
    "    print(f\"Output shape of a sample forward pass: {sample_output.shape}\") # Should be [BATCH_SIZE]\n",
    "    print(f\"Sample prediction value: {sample_output[0].item():.2f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during the model's forward pass test: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a269173",
   "metadata": {},
   "source": [
    "### **Step 5: Model Training Loop**\n",
    "\n",
    "We will now write a standard PyTorch training loop. The key component here is the implementation of the **weighted loss function**, which is central to your project's methodology as described in `TARGET-PRICE.md`.\n",
    "\n",
    "Instead of a simple Mean Squared Error (MSE), we will calculate a weighted MSE. Each sample's contribution to the total loss will be scaled by its `estimated_occupancy_rate` (the `sample_weight`). This forces the model to pay much more attention to fitting the prices of listings with high market activity.\n",
    "\n",
    "The loop will:\n",
    "1.  Define the optimizer (AdamW is a robust choice).\n",
    "2.  Iterate for a specified number of epochs.\n",
    "3.  For each epoch:\n",
    "    *   Run a **training phase** where we iterate through the `train_loader`, calculate the weighted loss, and update the model's weights via backpropagation.\n",
    "    *   Run a **validation phase** where we iterate through the `val_loader` with gradients turned off (`torch.no_grad()`) to calculate the loss on the validation set.\n",
    "4.  Print the training and validation loss after each epoch to monitor progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b997650d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Model Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Training]:  13%|█▎        | 33/260 [05:26<37:28,  9.90s/it, train_loss=2.31e+4]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m train_iterator \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [Training]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_iterator:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# 1. Get predictions\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# 2. Get targets and weights\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     targets \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_price\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[15], line 127\u001b[0m, in \u001b[0;36mAdditiveAxisModel.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# 4. Amenities Axis\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# The sentence_transformers library expects a dictionary for the forward pass\u001b[39;00m\n\u001b[1;32m    126\u001b[0m amenities_tokens \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamenities_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 127\u001b[0m amenities_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamenities_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamenities_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m amenities_embed \u001b[38;5;241m=\u001b[39m amenities_output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    129\u001b[0m p_amenities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamenities_subnet(amenities_embed)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:1175\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m   1169\u001b[0m             module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[1;32m   1170\u001b[0m         module_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1171\u001b[0m             key: value\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1173\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mforward_kwargs)\n\u001b[1;32m   1174\u001b[0m         }\n\u001b[0;32m-> 1175\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:261\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03mForward pass through the transformer model.\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m        - 'all_layer_embeddings': If the model outputs hidden states, contains embeddings from all layers\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    259\u001b[0m trans_features \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward_params}\n\u001b[0;32m--> 261\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m token_embeddings \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    263\u001b[0m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token_embeddings\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1000\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    997\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    998\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1000\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1013\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1014\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:650\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    646\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[1;32m    648\u001b[0m layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 650\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    661\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:588\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    585\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    586\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs \u001b[38;5;241m+\u001b[39m cross_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add cross attentions if we output attention weights\u001b[39;00m\n\u001b[0;32m--> 588\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/transformers/pytorch_utils.py:257\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:597\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    596\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 597\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:525\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 525\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    526\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    527\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/airbnb-project/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm # For a nice progress bar\n",
    "\n",
    "# --- Training Configuration ---\n",
    "N_EPOCHS = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# --- Instantiate Model, Optimizer, and Loss ---\n",
    "# Note: Re-instantiating the model to ensure we start with fresh, random weights\n",
    "model = AdditiveAxisModel(processor, device='cpu') \n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# We will implement the weighted MSE loss directly in the loop.\n",
    "\n",
    "# --- Training and Validation Loop ---\n",
    "print(\"\\n--- Starting Model Training ---\")\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    # --- Training Phase ---\n",
    "    model.train() # Set model to training mode\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Use tqdm for a progress bar over the training batches\n",
    "    train_iterator = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{N_EPOCHS} [Training]\")\n",
    "    \n",
    "    for batch in train_iterator:\n",
    "        # 1. Get predictions\n",
    "        predictions = model(batch)\n",
    "        \n",
    "        # 2. Get targets and weights\n",
    "        targets = batch['target_price'].to(model.device)\n",
    "        weights = batch['sample_weight'].to(model.device)\n",
    "        \n",
    "        # 3. Calculate weighted loss\n",
    "        # The core of the weighted learning strategy\n",
    "        loss = (weights * (predictions - targets)**2).mean()\n",
    "        \n",
    "        # 4. Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar description\n",
    "        train_iterator.set_postfix({'train_loss': train_loss / (train_iterator.n + 1)})\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad(): # Disable gradient calculation\n",
    "        val_iterator = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{N_EPOCHS} [Validation]\")\n",
    "        for batch in val_iterator:\n",
    "            predictions = model(batch)\n",
    "            targets = batch['target_price'].to(model.device)\n",
    "            weights = batch['sample_weight'].to(model.device)\n",
    "            \n",
    "            loss = (weights * (predictions - targets)**2).mean()\n",
    "            val_loss += loss.item()\n",
    "            val_iterator.set_postfix({'val_loss': val_loss / (val_iterator.n + 1)})\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    # --- Print Epoch Summary ---\n",
    "    print(f\"Epoch {epoch+1}/{N_EPOCHS} -> \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "print(\"\\n--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01b84d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airbnb-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
