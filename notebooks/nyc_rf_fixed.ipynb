{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [
        {
          "file_id": "1GzEIQetRkGUSQQvA097sYYBS-OD0_NU1",
          "timestamp": 1762230798565
        }
      ],
      "authorship_tag": "ABX9TyNOBfgjMKrZSDWLy2yqZvsW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --- Mount Google Drive (Colab) ---\n",
        "from google.colab import drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"‚úÖ Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not mount Google Drive. Error: {e}\")\n",
        "\n",
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/\n",
        "\n",
        "# -------------------------\n",
        "# Imports\n",
        "# -------------------------\n",
        "import os, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "\n",
        "# --- Import the shared data split function ---\n",
        "from data_processing import load_and_split_data\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "config = {\n",
        "    \"CITY\": \"nyc\",\n",
        "    \"SEED\": 42,\n",
        "    \"VAL_SIZE\": 0.05,\n",
        "    \"DRIVE_SAVE_PATH\": \"/content/drive/MyDrive/Colab_Notebooks/Airbnb_Project\",\n",
        "}\n",
        "\n",
        "# =========================================================\n",
        "# 1Ô∏è‚É£ Load the consistent split using load_and_split_data\n",
        "# =========================================================\n",
        "train_df, val_df, neighborhood_log_means, train_ids_set, val_ids_set = load_and_split_data(config)\n",
        "\n",
        "# =========================================================\n",
        "# 2Ô∏è‚É£ Build target as log deviation from neighborhood mean\n",
        "# =========================================================\n",
        "def attach_neigh_log_mean(df, mapping):\n",
        "    gmean = float(np.mean(list(mapping.values())))\n",
        "    out = df.copy()\n",
        "    out[\"neigh_log_mean\"] = out[\"neighbourhood_cleansed\"].map(mapping).fillna(gmean)\n",
        "    return out\n",
        "\n",
        "train_df = attach_neigh_log_mean(train_df, neighborhood_log_means)\n",
        "val_df = attach_neigh_log_mean(val_df, neighborhood_log_means)\n",
        "\n",
        "train_df[\"target_dev\"] = np.log1p(train_df[\"price\"]) - train_df[\"neigh_log_mean\"]\n",
        "val_df[\"target_dev\"]   = np.log1p(val_df[\"price\"])   - val_df[\"neigh_log_mean\"]\n",
        "\n",
        "# =========================================================\n",
        "# 3Ô∏è‚É£ Feature Engineering for RF\n",
        "# =========================================================\n",
        "TEXT_COL = \"amenities\"\n",
        "CAT_COLS = [\"property_type\", \"room_type\", \"neighbourhood_cleansed\"]\n",
        "NUM_COLS = [\n",
        "    \"accommodates\", \"review_scores_rating\", \"review_scores_cleanliness\",\n",
        "    \"review_scores_checkin\", \"review_scores_communication\",\n",
        "    \"review_scores_location\", \"review_scores_value\",\n",
        "    \"bedrooms\", \"beds\", \"bathrooms\"\n",
        "]\n",
        "\n",
        "def amenities_tokenizer(x):\n",
        "    if pd.isna(x):\n",
        "        return []\n",
        "    return [a.strip().lower() for a in str(x).split(\",\")]\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"amenities\", HashingVectorizer(\n",
        "            tokenizer=amenities_tokenizer,\n",
        "            n_features=256,\n",
        "            alternate_sign=False,\n",
        "            binary=True\n",
        "        ), TEXT_COL),\n",
        "        (\"categorical\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), CAT_COLS),\n",
        "        (\"numeric\", \"passthrough\", NUM_COLS),\n",
        "    ]\n",
        ")\n",
        "\n",
        "X_train = preprocessor.fit_transform(train_df)\n",
        "X_val   = preprocessor.transform(val_df)\n",
        "\n",
        "y_train = train_df[\"target_dev\"].values\n",
        "y_val   = val_df[\"target_dev\"].values\n",
        "\n",
        "# =========================================================\n",
        "# 4Ô∏è‚É£ Train RF with early stopping on Val MAPE\n",
        "# =========================================================\n",
        "print(\"\\nüöÄ Training Random Forest with early stopping on validation MAPE...\")\n",
        "\n",
        "patience = 3\n",
        "best_mape = float(\"inf\")\n",
        "patience_counter = 0\n",
        "chunk_size = 20\n",
        "max_estimators = 300\n",
        "best_model = None\n",
        "\n",
        "for n_trees in tqdm(range(chunk_size, max_estimators + 1, chunk_size), desc=\"Training Progress\", colour=\"blue\", ncols=90):\n",
        "    rf_partial = RandomForestRegressor(\n",
        "        n_estimators=n_trees,\n",
        "        max_depth=20,\n",
        "        min_samples_split=10,\n",
        "        min_samples_leaf=5,\n",
        "        max_features=\"sqrt\",\n",
        "        bootstrap=True,\n",
        "        random_state=config[\"SEED\"],\n",
        "        n_jobs=-1,\n",
        "        warm_start=True,\n",
        "    )\n",
        "    rf_partial.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on validation\n",
        "    preds_val_dev = rf_partial.predict(X_val)\n",
        "    preds_val_price = np.expm1(preds_val_dev + val_df[\"neigh_log_mean\"].values)\n",
        "    val_mape = mean_absolute_percentage_error(val_df[\"price\"], preds_val_price) * 100\n",
        "    tqdm.write(f\"Trees: {n_trees:>3} | Val MAPE: {val_mape:.2f}%\")\n",
        "\n",
        "    if val_mape < best_mape:\n",
        "        best_mape = val_mape\n",
        "        best_model = rf_partial\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"‚èπÔ∏è Early stopping at {n_trees} trees (Best MAPE: {best_mape:.2f}%).\")\n",
        "            break\n",
        "\n",
        "print(f\"\\n‚úÖ Best Validation MAPE: {best_mape:.2f}%\")\n",
        "\n",
        "# =========================================================\n",
        "# 5Ô∏è‚É£ Build Full Dataset for Predictions\n",
        "# =========================================================\n",
        "full_df = pd.concat([train_df, val_df], axis=0, ignore_index=True)\n",
        "\n",
        "def split_label(listing_id):\n",
        "    if listing_id in train_ids_set: return \"train\"\n",
        "    elif listing_id in val_ids_set: return \"val\"\n",
        "    return \"unknown\"\n",
        "\n",
        "full_df[\"split\"] = full_df[\"id\"].apply(split_label)\n",
        "full_df = attach_neigh_log_mean(full_df, neighborhood_log_means)\n",
        "\n",
        "X_full = preprocessor.transform(full_df)\n",
        "preds_dev_full = best_model.predict(X_full)\n",
        "full_df[\"predicted_price\"] = np.expm1(preds_dev_full + full_df[\"neigh_log_mean\"].values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591,
          "referenced_widgets": [
            "b4b4210284eb49f2896c819f82a2f3e5",
            "2f79fd682ad04367b6e8000afae6bdcd",
            "f4bfdfbb022a4791848d9885e884f8db",
            "067ded8d257842bfad8ea508b63a5ca0",
            "db61a86f8d27468381137a446aa0d653",
            "e78b065493064c68ac96313414654ba5",
            "23951de63e0c41d785ec748fc30057ff",
            "b658c9f7c6c4415881f3663e56ba72e2",
            "e421cb49b3534bd88a9ee9b6dfd26dac",
            "f752cc7512a94adf8e2d400b8b3bf045",
            "e071bc35ab3b4e7c87fae2d15e2b4a98"
          ]
        },
        "id": "IeEXxHY5VJbD",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1762387982137,
          "user_tz": 300,
          "elapsed": 2711588,
          "user": {
            "displayName": "Ma Junyu",
            "userId": "13374034762989613912"
          }
        },
        "outputId": "a5aea675-b679-4f29-e95f-a6b2397719a8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Google Drive mounted successfully.\n",
            "/content/drive/MyDrive/Colab Notebooks\n",
            "Data split: 121,187 train records, 6,403 validation records.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ Training Random Forest with early stopping on validation MAPE...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training Progress:   0%|                                           | 0/15 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4b4210284eb49f2896c819f82a2f3e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trees:  20 | Val MAPE: 30.19%\n",
            "Trees:  40 | Val MAPE: 29.80%\n",
            "Trees:  60 | Val MAPE: 29.72%\n",
            "Trees:  80 | Val MAPE: 29.64%\n",
            "Trees: 100 | Val MAPE: 29.63%\n",
            "Trees: 120 | Val MAPE: 29.54%\n",
            "Trees: 140 | Val MAPE: 29.56%\n",
            "Trees: 160 | Val MAPE: 29.48%\n",
            "Trees: 180 | Val MAPE: 29.41%\n",
            "Trees: 200 | Val MAPE: 29.40%\n",
            "Trees: 220 | Val MAPE: 29.46%\n",
            "Trees: 240 | Val MAPE: 29.42%\n",
            "Trees: 260 | Val MAPE: 29.42%\n",
            "‚èπÔ∏è Early stopping at 260 trees (Best MAPE: 29.40%).\n",
            "\n",
            "‚úÖ Best Validation MAPE: 29.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6Ô∏è‚É£ Artifact 1: Save Predictions DataFrame\n",
        "# =========================================================\n",
        "required_cols = [\n",
        "    \"id\", \"name\", \"latitude\", \"longitude\", \"neighbourhood_cleansed\",\n",
        "    \"month\", \"split\", \"price\", \"predicted_price\"\n",
        "]\n",
        "predictions_df = full_df[required_cols].copy()\n",
        "\n",
        "os.makedirs(config[\"DRIVE_SAVE_PATH\"], exist_ok=True)\n",
        "pred_path = os.path.join(config[\"DRIVE_SAVE_PATH\"], f\"{config['CITY']}_rf_model_predictions.parquet\")\n",
        "predictions_df.to_parquet(pred_path, index=False)\n",
        "print(f\"\\nüì¶ Saved {config['CITY']}_rf_model_predictions.parquet to:\\n{pred_path}\")\n",
        "\n",
        "# =========================================================\n",
        "# 7Ô∏è‚É£ Artifact 2: Feature Importances\n",
        "# =========================================================\n",
        "cat_encoder = preprocessor.named_transformers_[\"categorical\"]\n",
        "cat_feature_names = list(cat_encoder.get_feature_names_out(CAT_COLS))\n",
        "n_hash = preprocessor.named_transformers_[\"amenities\"].n_features\n",
        "amenity_feature_names = [f\"amenity_{i}\" for i in range(n_hash)]\n",
        "num_feature_names = NUM_COLS\n",
        "feature_names = amenity_feature_names + cat_feature_names + num_feature_names\n",
        "\n",
        "importances = best_model.feature_importances_\n",
        "importance_df = pd.DataFrame({\"feature\": feature_names, \"importance\": importances}).sort_values(\"importance\", ascending=False)\n",
        "\n",
        "imp_path = os.path.join(config[\"DRIVE_SAVE_PATH\"], f\"{config['CITY']}_rf_feature_importances.csv\")\n",
        "importance_df.to_csv(imp_path, index=False)\n",
        "print(f\"üìä Saved {config['CITY']}_rf_feature_importances.csv to:\\n{imp_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1P3FUr1VLcW",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1762387982732,
          "user_tz": 300,
          "elapsed": 582,
          "user": {
            "displayName": "Ma Junyu",
            "userId": "13374034762989613912"
          }
        },
        "outputId": "3e286801-001a-4032-effa-2406cc802b97"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üì¶ Saved nyc_rf_model_predictions.parquet to:\n",
            "/content/drive/MyDrive/Colab_Notebooks/Airbnb_Project/nyc_rf_model_predictions.parquet\n",
            "üìä Saved nyc_rf_feature_importances.csv to:\n",
            "/content/drive/MyDrive/Colab_Notebooks/Airbnb_Project/nyc_rf_feature_importances.csv\n"
          ]
        }
      ]
    }
  ]
}