{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "123ef2b6",
   "metadata": {},
   "source": [
    "### 0. Setup and Installations\n",
    "\n",
    "This cell prepares the Google Colab environment by mounting Google Drive, changing the working directory to our project folder to ensure all custom modules can be imported, installing the required Python packages, and handling Hugging Face authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5867b3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Mount Google Drive ---\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# --- Change Directory to Project Folder ---\n",
    "# This is a crucial step that makes all local imports work seamlessly\n",
    "import os\n",
    "# IMPORTANT: Make sure this path matches the location of your project folder in Google Drive\n",
    "PROJECT_PATH = '/content/drive/MyDrive/Airbnb_Price_Project'\n",
    "os.chdir(PROJECT_PATH)\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110448be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hugging Face Authentication ---\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "print(\"\\nAttempting Hugging Face login...\")\n",
    "try:\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"Hugging Face login successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not log in. Please ensure 'HF_TOKEN' is a valid secret. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f17212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install Dependencies ---\n",
    "!pip install pandas\n",
    "!pip install pyarrow\n",
    "!pip install sentence-transformers\n",
    "!pip install scikit-learn\n",
    "!pip install torch\n",
    "!pip install tqdm\n",
    "!pip install transformers\n",
    "!pip install matplotlib\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933134c6",
   "metadata": {},
   "source": [
    "### 1. Imports and Global Configuration\n",
    "\n",
    "Here, we import all necessary functions and classes from our custom Python scripts. We also load the central `config` dictionary, set the global random seed for reproducibility, and confirm which compute device (`cuda` or `cpu`) is being used for the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48182785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "# Import from our custom scripts\n",
    "from config import config\n",
    "from utils import set_seed, plot_target_distributions, plot_training_history\n",
    "from data_processing import load_and_split_data, FeatureProcessor, create_dataloaders\n",
    "from model import AdditiveAxisModel\n",
    "from train import train_model, save_artifacts\n",
    "from build_app_dataset import build_dataset\n",
    "\n",
    "# Set the seed for the entire notebook\n",
    "set_seed(config['SEED'])\n",
    "\n",
    "# Optional: choose CITY to be \"nyc\" or \"toronto\"\n",
    "# config[\"CITY\"] = \"nyc\"\n",
    "\n",
    "# Print every item in the config dictionary\n",
    "print(\"\\nConfiguration Settings:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Confirm the device\n",
    "print(f\"\\nUsing device: {config['DEVICE']}\")\n",
    "if config['DEVICE'] == 'cuda':\n",
    "    print(f\"GPU Name: {torch.cuda.get_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7fdda7",
   "metadata": {},
   "source": [
    "### 2. Load and Split Data\n",
    "\n",
    "We now load the raw dataset and perform the stratified group split using the `load_and_split_data` function. This ensures that all records for a given `listing_id` are isolated to either the training or validation set, preventing data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358e85f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split the data according to the new stratified group logic\n",
    "train_df, val_df, neighborhood_log_means, train_ids, val_ids = load_and_split_data(config)\n",
    "\n",
    "# Print the shapes to confirm the split\n",
    "print(f\"\\nTraining DataFrame shape: {train_df.shape}\")\n",
    "print(f\"Validation DataFrame shape: {val_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccb87b9",
   "metadata": {},
   "source": [
    "### 3. Visualize Target Distributions\n",
    "\n",
    "This cell calls the `plot_target_distributions` function to generate a set of visualizations. This serves as a critical sanity check to confirm that our stratified split has produced training and validation sets with similar distributions for price, log-price, and our final target variable (log-price deviation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89466292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distributions of the target variable and its components\n",
    "plot_target_distributions(train_df, val_df, neighborhood_log_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feccd725",
   "metadata": {},
   "source": [
    "### 4. Process Features\n",
    "\n",
    "With the data split and verified, we now prepare it for the model. We instantiate the `FeatureProcessor`, fit it exclusively on the training data to learn vocabularies and scaling parameters, and then use the fitted processor to transform both the training and validation sets into numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a20bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and fit the feature processor\n",
    "processor = FeatureProcessor(config)\n",
    "processor.fit(train_df)\n",
    "\n",
    "# Transform both datasets into feature dictionaries\n",
    "train_features = processor.transform(train_df, neighborhood_log_means)\n",
    "val_features = processor.transform(val_df, neighborhood_log_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ed7fde",
   "metadata": {},
   "source": [
    "### 5. Instantiate Model and DataLoaders\n",
    "\n",
    "We create the core PyTorch objects for training. First, we instantiate our `AdditiveAxisModel`, then call the `count_parameters()` method to get a detailed summary of its architecture. Finally, we create the `train_loader` and `val_loader` which will handle batching and data shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a052dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = AdditiveAxisModel(processor, config)\n",
    "\n",
    "# Print the breakdown of trainable and frozen parameters\n",
    "model.count_parameters()\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_loader, val_loader = create_dataloaders(train_features, val_features, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df4557b",
   "metadata": {},
   "source": [
    "### 6. Define Optimizer and Scheduler\n",
    "\n",
    "Here, we define the optimization components. We create two separate parameter groups to apply a much lower learning rate to the pre-trained text transformer's fine-tuned layer. We then instantiate the AdamW optimizer and the `ReduceLROnPlateau` scheduler, which will automatically reduce the learning rate if validation performance stagnates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df15ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parameter groups for differential learning rates\n",
    "transformer_params = model.text_transformer.parameters()\n",
    "other_params = [p for n, p in model.named_parameters() if 'text_transformer' not in n]\n",
    "\n",
    "# Instantiate the optimizer\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': other_params, 'lr': config['LEARNING_RATE']},\n",
    "    {'params': transformer_params, 'lr': config['TRANSFORMER_LEARNING_RATE']}\n",
    "])\n",
    "\n",
    "# Instantiate the learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=config['SCHEDULER_FACTOR'],\n",
    "    patience=config['SCHEDULER_PATIENCE']\n",
    ")\n",
    "print(\"Optimizer and Scheduler have been defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025d2abe",
   "metadata": {},
   "source": [
    "### 7. Train the Model\n",
    "\n",
    "This is the main training step. We call the `train_model` function, which encapsulates the entire training loop, including forward/backward passes, optimization, evaluation, and early stopping. The best performing model state and a history of performance metrics are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5c8b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training loop\n",
    "trained_model, history_df = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb4bd13",
   "metadata": {},
   "source": [
    "### 8. Analyze Training History\n",
    "\n",
    "Immediately after training, we visualize the results by passing the `history_df` to the `plot_training_history` function. This dual-axis plot allows us to inspect the training/validation loss curves and the validation MAPE to assess for overfitting and confirm that the model converged properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c95e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss/MAPE curves\n",
    "plot_training_history(history_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3311c0ea",
   "metadata": {},
   "source": [
    "### 9. Save Model Artifacts\n",
    "\n",
    "Now that the model is trained, we save all the essential components—the trained model's state dictionary, the fitted feature processor, and the configuration—to a single timestamped `.pt` file. This allows us to easily load and reuse the entire pipeline for inference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c13ba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model, processor, and config to a file\n",
    "saved_artifacts_path = save_artifacts(trained_model, processor, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced1156b",
   "metadata": {},
   "source": [
    "### 10. Build and Save Final Application Dataset\n",
    "\n",
    "This is the final \"production\" step. We package the necessary objects from our session into a dictionary and pass them to the `build_dataset` function. This function will perform the computationally expensive tasks of augmenting the dataset for all months and running inference on the entire panel, creating the self-contained database for our Streamlit application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c9966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package the necessary objects for the build process\n",
    "artifacts_for_build = {\n",
    "    'model': trained_model,\n",
    "    'processor': processor,\n",
    "    'train_ids': train_ids,\n",
    "    'val_ids': val_ids\n",
    "}\n",
    "\n",
    "# Run the build process\n",
    "build_dataset(artifacts_for_build)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec201c1",
   "metadata": {},
   "source": [
    "### 11. Step 10: Verify Final Dataset\n",
    "\n",
    "As a final sanity check, we load the application database that was just created. We then display its schema using `.info()` and a few sample rows with `.head()` to confirm that it has the correct structure, includes all the new `p_*`, `pm_*`, and `h_*` columns, and is ready for use in the analysis notebook and the final web application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e416cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the path to the newly created app database\n",
    "app_data_path = os.path.join(config['DRIVE_SAVE_PATH'], 'app_data', f\"{config['CITY']}_app_database.parquet\")\n",
    "\n",
    "# Load and inspect the final dataset\n",
    "print(f\"Loading final app database from: {app_data_path}\")\n",
    "final_app_df = pd.read_parquet(app_data_path)\n",
    "\n",
    "print(\"\\n--- Final App Database Info ---\")\n",
    "final_app_df.info()\n",
    "\n",
    "print(\"\\n--- Final App Database Head ---\")\n",
    "display(final_app_df.head())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
