{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "376f8740",
   "metadata": {},
   "source": [
    "# 3. Final Additive Explainable Model\n",
    "\n",
    "### Objective\n",
    "\n",
    "This notebook details the training and finalization of our primary model: the `AdditiveModel`. This model is the core of our project, designed specifically for **explainability**.\n",
    "\n",
    "Its unique architecture predicts an Airbnb's price not as a single opaque number, but as a sum of contributions from six distinct feature axes: Location, Size & Capacity, Quality, Amenities, Description, and Seasonality. This allows us to understand *why* a listing is priced the way it is.\n",
    "\n",
    "After training, we will use this model to generate the final, enriched dataset that will power our interactive Streamlit application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd10930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive, userdata\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "print(\"--- Setting up Environment ---\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# IMPORTANT: Make sure this path matches your project folder\n",
    "PROJECT_PATH = '/content/drive/MyDrive/Airbnb_Price_Project'\n",
    "os.chdir(PROJECT_PATH)\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# --- Standard and Third-Party Library Imports ---\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Imports from Custom Project Scripts ---\n",
    "print(\"\\n--- Importing Custom Modules ---\")\n",
    "from config import config\n",
    "from data_processing import load_and_split_data, FeatureProcessor, create_dataloaders\n",
    "from model import AdditiveModel # Import the final AdditiveModel\n",
    "from train import train_model, evaluate_model\n",
    "from build_app_dataset import build_dataset # Import the final dataset builder\n",
    "\n",
    "print(\"\\nSetup and imports complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9228f42",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "As with our baseline model, we begin by loading the data and applying our stratified group split to create reliable training and validation sets. We then fit our `FeatureProcessor` on the training data to learn all necessary transformations, ensuring a consistent and leak-free data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d2a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split the data\n",
    "train_df, val_df, neighborhood_log_means, train_ids, val_ids = load_and_split_data(config)\n",
    "\n",
    "# Instantiate and fit the feature processor on the training data\n",
    "processor = FeatureProcessor(config)\n",
    "processor.fit(train_df)\n",
    "\n",
    "# Transform both datasets into feature dictionaries\n",
    "train_features = processor.transform(train_df, neighborhood_log_means)\n",
    "val_features = processor.transform(val_df, neighborhood_log_means)\n",
    "\n",
    "# Create the PyTorch DataLoaders\n",
    "train_loader, val_loader = create_dataloaders(train_features, val_features, config)\n",
    "\n",
    "print(\"\\nData pipeline complete. DataLoaders are ready for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ede125",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "\n",
    "We instantiate our final `AdditiveModel`. A key aspect of this model is its use of a pre-trained `SentenceTransformer` for understanding text features. To improve its performance on our specific dataset, we \"fine-tune\" it.\n",
    "\n",
    "This is achieved by using **differential learning rates**:\n",
    "- The newly added MLP layers of our model train with a standard learning rate (e.g., `1e-3`).\n",
    "- The pre-trained transformer layers are trained with a much smaller learning rate (e.g., `1e-5`).\n",
    "\n",
    "This allows the transformer to adapt to the nuances of Airbnb descriptions without catastrophically forgetting the general language knowledge it already possesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the AdditiveModel\n",
    "model = AdditiveModel(processor, config)\n",
    "model.to(config['DEVICE'])\n",
    "\n",
    "# --- Create parameter groups for differential learning rates ---\n",
    "# Parameters from the pre-trained text transformer\n",
    "transformer_params = model.text_transformer.parameters()\n",
    "# All other parameters in our model (MLPs, embeddings)\n",
    "other_params = [p for n, p in model.named_parameters() if 'text_transformer' not in n]\n",
    "\n",
    "# Instantiate the optimizer with two parameter groups\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': other_params, 'lr': config['LEARNING_RATE'], 'weight_decay': config['WEIGHT_DECAY']},\n",
    "    {'params': transformer_params, 'lr': config['TRANSFORMER_LEARNING_RATE']}\n",
    "])\n",
    "\n",
    "# Instantiate the learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=config['SCHEDULER_FACTOR'],\n",
    "    patience=config['SCHEDULER_PATIENCE']\n",
    ")\n",
    "\n",
    "print(f\"AdditiveModel and optimizer with differential learning rates have been initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4190e78b",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "We now pass all components to our reusable `train_model` function. It will handle the entire training process, including early stopping, and return the best-performing model state along with a history of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43120dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model, history_df = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a926910a",
   "metadata": {},
   "source": [
    "## Final Evaluation\n",
    "\n",
    "With the best model checkpoint loaded, we perform a final evaluation on the validation set to confirm its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47f26f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Final Model Evaluation ---\")\n",
    "_, final_val_mape = evaluate_model(trained_model, val_loader, config['DEVICE'])\n",
    "final_val_rmse = np.sqrt(evaluate_model(trained_model, val_loader, config['DEVICE'])[0])\n",
    "\n",
    "final_metrics = {\n",
    "    \"val_rmse\": final_val_rmse,\n",
    "    \"val_mape\": final_val_mape\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"{'Final Additive Model Performance Metrics':^50}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Validation RMSE: {final_metrics['val_rmse']:.4f}\")\n",
    "print(f\"Validation MAPE: {final_metrics['val_mape'] * 100:.2f}%\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fcbee4",
   "metadata": {},
   "source": [
    "## Building the Final Application Dataset\n",
    "\n",
    "This is the final \"production\" step of our modeling pipeline. With our trained `AdditiveModel`, we now execute the `build_dataset` function from our `build_app_dataset.py` script.\n",
    "\n",
    "This function performs several computationally intensive, one-time tasks:\n",
    "1.  Creates a complete \"panel\" of every listing for all 12 months.\n",
    "2.  Runs inference on this entire panel using our model.\n",
    "3.  Captures not only the final `predicted_price` but also all the explainable components (`p_*` for log-contributions, `pm_*` for multiplicative factors) and the hidden state vectors (`h_*`) needed for the similarity search.\n",
    "4.  Saves this single, enriched, and self-contained DataFrame to a Parquet file, which is the sole data artifact required by our Streamlit application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95f8dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Starting build process for the application dataset ---\")\n",
    "\n",
    "# Pass all necessary objects to the build function\n",
    "build_dataset(\n",
    "    model=trained_model,\n",
    "    processor=processor,\n",
    "    config=config,\n",
    "    train_ids=train_ids,\n",
    "    val_ids=val_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7eb52f",
   "metadata": {},
   "source": [
    "## Save Core Artifacts\n",
    "\n",
    "While the main data artifact has already been created by the build script, we also save the core modeling components for reproducibility and potential future analysis. This includes the trained model's state dictionary and the fitted `FeatureProcessor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814e3ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "print(\"--- Saving core model artifacts ---\")\n",
    "\n",
    "# 1. Define paths\n",
    "timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "artifacts_dir = os.path.join(config['DRIVE_SAVE_PATH'], f\"additive_{timestamp}\")\n",
    "os.makedirs(artifacts_dir, exist_ok=True)\n",
    "\n",
    "model_save_path = os.path.join(artifacts_dir, \"additive_model.pt\")\n",
    "processor_save_path = os.path.join(artifacts_dir, \"feature_processor.pkl\")\n",
    "\n",
    "# 2. Save the model state\n",
    "torch.save({\n",
    "    'model_state_dict': trained_model.state_dict(),\n",
    "    'final_metrics': final_metrics\n",
    "}, model_save_path)\n",
    "print(f\"Model and metrics saved to: {model_save_path}\")\n",
    "\n",
    "# 3. Save the feature processor\n",
    "with open(processor_save_path, 'wb') as f:\n",
    "    pickle.dump(processor, f)\n",
    "print(f\"Feature processor saved to: {processor_save_path}\")\n",
    "\n",
    "print(\"\\nAll additive model artifacts have been saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
