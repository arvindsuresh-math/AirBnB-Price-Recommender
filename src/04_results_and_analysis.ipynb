{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e4d55e7",
   "metadata": {},
   "source": [
    "# 4. Model Comparison and Results Analysis\n",
    "\n",
    "### Objective\n",
    "\n",
    "This notebook is the final step in our analysis. Here, we bring together the results from our three distinct modeling approaches to perform a comprehensive comparison:\n",
    "1.  **Random Forest:** A classic, powerful tree-based model.\n",
    "2.  **Baseline Neural Network:** A robust, fully-connected deep learning model.\n",
    "3.  **Additive Neural Network:** Our final, interpretable deep learning model.\n",
    "\n",
    "We will compare their overall predictive performance, investigate what features each model found important, and, most critically, perform a deep dive into the **explainability** of the `AdditiveModel` to demonstrate its unique value for a price recommendation tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7ad1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Environment Setup (for Google Colab) ---\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "print(\"--- Setting up Environment ---\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "PROJECT_PATH = '/content/drive/MyDrive/Airbnb_Price_Project'\n",
    "os.chdir(PROJECT_PATH)\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# --- Standard and Third-Party Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Imports from Custom Project Scripts ---\n",
    "print(\"\\n--- Importing Custom Modules ---\")\n",
    "from config import config\n",
    "from plotting import (\n",
    "    plot_predictions_vs_actual,\n",
    "    plot_mape_distribution,\n",
    "    plot_ablation_results,\n",
    "    plot_additive_contributions\n",
    ")\n",
    "\n",
    "# --- Plotting Configuration ---\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 7)\n",
    "\n",
    "print(\"\\nSetup and imports complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a19628",
   "metadata": {},
   "source": [
    "## Loading All Model Artifacts\n",
    "\n",
    "We begin by loading all the necessary data artifacts that were generated and saved by our previous notebooks. This includes:\n",
    "- The prediction DataFrames for each of the three models.\n",
    "- The results DataFrame from the `AdditiveModel` ablation study.\n",
    "\n",
    "**Note:** For this notebook to run, you must first execute the other training notebooks (`01`, `02`, `03`) and the separate Random Forest training script to generate these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2739aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Loading model prediction artifacts ---\")\n",
    "\n",
    "# NOTE: You will need to find the correct timestamped folder for your artifacts\n",
    "# For example: '/content/drive/MyDrive/Airbnb_Price_Project/artifacts/baseline_20251104_103000/'\n",
    "BASELINE_ARTIFACT_PATH = \"\" # TODO: Fill in the path to your baseline artifacts folder\n",
    "ADDITIVE_ARTIFACT_PATH = \"\" # TODO: Fill in the path to your additive model artifacts folder\n",
    "RF_ARTIFACT_PATH = \"\"       # TODO: Fill in the path to your Random Forest artifacts folder\n",
    "ABLATION_RESULTS_PATH = \"\"  # TODO: Fill in the direct path to your ablation_results.csv file\n",
    "\n",
    "try:\n",
    "    # Load predictions only for listings in the validation set for a fair comparison\n",
    "    baseline_preds_df = pd.read_parquet(os.path.join(BASELINE_ARTIFACT_PATH, \"baseline_model_predictions.parquet\"))\n",
    "    baseline_val_df = baseline_preds_df[baseline_preds_df['split'] == 'val'].dropna(subset=['price'])\n",
    "\n",
    "    additive_preds_df = pd.read_parquet(os.path.join(ADDITIVE_ARTIFACT_PATH, \"additive_model_predictions.parquet\"))\n",
    "    additive_val_df = additive_preds_df[additive_preds_df['split'] == 'val'].dropna(subset=['price'])\n",
    "    \n",
    "    # Assuming the RF model predictions file is named similarly\n",
    "    rf_preds_df = pd.read_parquet(os.path.join(RF_ARTIFACT_PATH, \"rf_model_predictions.parquet\"))\n",
    "    rf_val_df = rf_preds_df[rf_preds_df['split'] == 'val'].dropna(subset=['price'])\n",
    "\n",
    "    ablation_df = pd.read_csv(ABLATION_RESULTS_PATH)\n",
    "    \n",
    "    print(\"All prediction and result files loaded successfully.\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: Could not find a required file. Please check your paths.\")\n",
    "    print(e)\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": rf_val_df,\n",
    "    \"Baseline NN\": baseline_val_df,\n",
    "    \"Additive NN\": additive_val_df\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efdad66",
   "metadata": {},
   "source": [
    "## Overall Performance Comparison\n",
    "\n",
    "We first compare the models on their core predictive accuracy. We will use two key visualizations:\n",
    "\n",
    "1.  **True vs. Predicted Price:** A scatter plot that shows how well the model's predictions align with the actual prices. A perfect model would have all points lying on the y=x line.\n",
    "2.  **MAPE Distribution:** A histogram showing the distribution of the Mean Absolute Percentage Error for each prediction. This is more informative than a single average MAPE value, as it reveals if a model makes a few very large errors or many small ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8c2502",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(24, 7))\n",
    "fig.suptitle('Model Performance: True vs. Predicted Prices (Validation Set)', fontsize=20)\n",
    "\n",
    "for ax, (model_name, df) in zip(axes, models.items()):\n",
    "    plot_predictions_vs_actual(df, model_name, ax)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc602ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(24, 7), sharey=True)\n",
    "fig.suptitle('Model Performance: Distribution of Prediction Errors (MAPE)', fontsize=20)\n",
    "\n",
    "for ax, (model_name, df) in zip(axes, models.items()):\n",
    "    plot_mape_distribution(df.copy(), model_name, ax) # Pass copy to avoid SettingWithCopyWarning\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18f8012",
   "metadata": {},
   "source": [
    "## Feature and Axis Importance\n",
    "\n",
    "Next, we investigate *what* each model learned was important for predicting price.\n",
    "\n",
    "-   **Ablation Study:** For our `AdditiveModel`, the results from the ablation study provide a powerful, direct measure of axis importance. A larger increase in error when an axis is removed implies it is more critical to the model's performance.\n",
    "-   **Random Forest:** We also look at the classic Gini importance from our Random Forest model as a point of comparison from a non-neural network approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caed33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Additive Model: Ablation Study Results ---\")\n",
    "plot_ablation_results(ablation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee202831",
   "metadata": {},
   "source": [
    "## Deep Dive: Explaining a Single Prediction\n",
    "\n",
    "The standout feature of the `AdditiveModel` is its ability to explain its own predictions. Here, we demonstrate this by selecting a single, high-priced listing from the validation set and visualizing how the model constructed its price recommendation.\n",
    "\n",
    "The plot below shows the **neighborhood average price** as the baseline. Each bar then represents the positive or negative price adjustment contributed by that specific feature axis, leading to the final predicted price. This level of transparency is invaluable for a real-world price recommendation tool, as it builds trust and provides actionable insights to hosts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d873f8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an interesting listing from the validation set (e.g., one of the most expensive)\n",
    "selected_listing = additive_val_df.sort_values('price', ascending=False).iloc[0]\n",
    "\n",
    "print(f\"--- Explaining Prediction for Listing: '{selected_listing['name']}' ---\")\n",
    "print(f\"Neighborhood: {selected_listing['neighbourhood_cleansed']}\")\n",
    "print(f\"Actual Price: ${selected_listing['price']:.2f}\")\n",
    "print(f\"Predicted Price: ${selected_listing['predicted_price']:.2f}\\n\")\n",
    "\n",
    "# Use our plotting function to generate the breakdown\n",
    "plot_additive_contributions(selected_listing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bc7ab1",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "**(This is where you will write your final project summary based on the plots above)**\n",
    "\n",
    "*Example conclusion:*\n",
    "\n",
    "Across all models, we observed strong predictive performance, with the Additive Neural Network achieving a validation MAPE of XX.X%, comparable to the Baseline NN and the Random Forest.\n",
    "\n",
    "The key differentiator is the `AdditiveModel`'s inherent **explainability**. The ablation study revealed that **Location** and **Size/Capacity** were the most critical axes for model performance, with their removal causing the largest increase in prediction error. This aligns with our domain knowledge of real estate.\n",
    "\n",
    "Most importantly, the ability to decompose any single prediction into its constituent parts—as demonstrated in the final plot—is the primary value of this project. While all three models can answer \"What is the predicted price?\", only the `AdditiveModel` can effectively answer \"**Why is that the predicted price?**\". This makes it the superior choice for a practical price recommendation tool, as it provides hosts with transparent, actionable insights into how their listing's characteristics translate to market value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca142e93",
   "metadata": {},
   "source": [
    "### Executive Summary\n",
    "\n",
    "To successfully run `04_results_and_analysis.ipynb`, you will need to produce **two main artifacts** from your Random Forest training script/notebook:\n",
    "\n",
    "1.  **A Predictions DataFrame:** A Parquet file named `rf_model_predictions.parquet` containing predictions for the *entire dataset*, structured identically to the output from your baseline neural network notebook.\n",
    "2.  **A Feature Importances DataFrame:** A small CSV file named `rf_feature_importances.csv` that lists each feature used by the model and its corresponding importance score.\n",
    "\n",
    "---\n",
    "\n",
    "### Artifact 1: The Predictions DataFrame (`rf_model_predictions.parquet`)\n",
    "\n",
    "This is the most critical artifact. Its purpose is to provide the raw data for the performance comparison plots (True vs. Predicted and MAPE Distribution).\n",
    "\n",
    "**Content:** The DataFrame must contain predictions for **every single row** in the full panel dataset you would generate (i.e., every listing for all 12 months). This ensures it has the same dimensions as the DataFrames from your other models.\n",
    "\n",
    "**Required Columns:**\n",
    "\n",
    "| Column Name            | Data Type     | Purpose & How to Generate It                                                                                                                                                             |\n",
    "| :--------------------- | :------------ | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **`id`**               | `int`         | **Listing Identifier.** Comes directly from the original data.                                                                                                                             |\n",
    "| **`name`**             | `str`         | **Listing Name.** Comes directly from the original data. Useful for context.                                                                                                             |\n",
    "| **`latitude`**         | `float`       | **GPS Coordinate.** Comes directly from the original data.                                                                                                                               |\n",
    "| **`longitude`**        | `float`       | **GPS Coordinate.** Comes directly from the original data.                                                                                                                               |\n",
    "| **`neighbourhood_cleansed`** | `str`         | **Neighborhood Name.** Comes directly from the original data.                                                                                                                          |\n",
    "| **`month`**            | `int`         | **Temporal Identifier (1-12).** Comes from the panel generation step.                                                                                                                      |\n",
    "| **`split`**            | `str`         | **Crucial for Fair Evaluation.** Must contain `'train'` or `'val'` for each listing, corresponding to the *exact same split* used for the neural networks.                                     |\n",
    "| **`price`**            | `float`       | **Ground Truth.** The actual historical price. This is your target variable. It will have `NaN`s for months without data.                                                                      |\n",
    "| **`predicted_price`**  | `float`       | **Model's Output.** The price prediction generated by calling `model.predict()` on the feature-engineered data for every row.                                                              |\n",
    "\n",
    "---\n",
    "\n",
    "### Artifact 2: Feature Importances (`rf_feature_importances.csv`)\n",
    "\n",
    "The purpose of this artifact is to allow you to create a bar chart in Notebook 4 showing which features the Random Forest found most predictive, which you can then compare (conceptually) to the results of your ablation study.\n",
    "\n",
    "**Content:** A simple two-column DataFrame.\n",
    "\n",
    "**Required Columns:**\n",
    "\n",
    "| Column Name     | Data Type | Purpose & How to Generate It                                                                                                                                          |\n",
    "| :-------------- | :-------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **`feature`**   | `str`     | **The name of the feature.** After you have prepared your data for the RF (e.g., after one-hot encoding), this is the list of your final column names.                   |\n",
    "| **`importance`**| `float`   | **The Gini importance score.** This is extracted from the trained model object via the `model.feature_importances_` attribute.                                          |\n",
    "\n",
    "**Crucial Note:** You must save the feature names and the importance scores together, ensuring the order is correct. The `i`-th score in `model.feature_importances_` corresponds to the `i`-th column name in the DataFrame you used for training.\n",
    "\n",
    "---\n",
    "\n",
    "### The Most Important Prerequisite: A Consistent Data Split\n",
    "\n",
    "For your entire project to be scientifically valid, the Random Forest model **must be trained and validated on the exact same data split as your neural network models.**\n",
    "\n",
    "This means your Random Forest training script **must** begin by calling the `load_and_split_data` function from `data_processing.py`.\n",
    "\n",
    "```python\n",
    "# In your Random Forest training script...\n",
    "from data_processing import load_and_split_data\n",
    "\n",
    "# Use the exact same function to get the same train/val listing IDs\n",
    "train_df, val_df, _, _, _ = load_and_split_data(config)\n",
    "```\n",
    "\n",
    "You will then train your Random Forest `model` **only on `train_df`** and evaluate its performance **only on `val_df`**.\n",
    "\n",
    "### Putting It All Together: Pseudo-Code for Your RF Script\n",
    "\n",
    "Your Random Forest training script should follow these logical steps:\n",
    "\n",
    "1.  **Load Data using the Shared Function:** Call `load_and_split_data(config)` to get `train_df` and `val_df`.\n",
    "2.  **Perform RF-Specific Feature Engineering:**\n",
    "    *   Unlike the NNs, a Random Forest cannot handle raw text or categorical strings. You will need to perform transformations like:\n",
    "        *   **One-Hot Encoding** for `property_type`, `room_type`, `neighbourhood_cleansed`.\n",
    "        *   Dropping high-cardinality or raw text columns like `name`, `description`, `amenities`.\n",
    "    *   Keep track of the final list of feature column names after this step.\n",
    "3.  **Train the Model:**\n",
    "    *   `model = RandomForestRegressor(...)`\n",
    "    *   `model.fit(X_train, y_train)` (where `X_train` and `y_train` are derived *only* from `train_df`).\n",
    "4.  **Generate Predictions for the *Full* Dataset:**\n",
    "    *   Create the full panel DataFrame just like you do in the baseline notebook.\n",
    "    *   Apply the *same* RF-specific feature engineering to this full panel.\n",
    "    *   `full_predictions = model.predict(X_full_panel)`\n",
    "5.  **Create and Save the Predictions DataFrame:**\n",
    "    *   Create a new DataFrame.\n",
    "    *   Copy over the essential columns (`id`, `name`, `price`, `split`, etc.) from the full panel.\n",
    "    *   Add the `predicted_price` column using `full_predictions`.\n",
    "    *   `predictions_df.to_parquet(\"rf_model_predictions.parquet\")`\n",
    "6.  **Extract and Save Feature Importances:**\n",
    "    *   `importances = model.feature_importances_`\n",
    "    *   `feature_names = list_of_column_names_from_step_2`\n",
    "    *   `importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})`\n",
    "    *   `importance_df.sort_values('importance', ascending=False).to_csv(\"rf_feature_importances.csv\")`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
