{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d71cad3",
   "metadata": {},
   "source": [
    "# 1. Baseline Neural Network Model\n",
    "\n",
    "### Objective\n",
    "\n",
    "In this notebook, we train and evaluate a fully-connected deep learning model. This model will serve as a robust performance baseline against which we can compare our more complex, interpretable `AdditiveModel`.\n",
    "\n",
    "The architecture is a standard Multi-Layer Perceptron (MLP) that takes all available features—including location, size, quality, and text embeddings—concatenates them into a single vector, and processes them through several layers to predict the final log-price deviation. Regularization techniques like Dropout, Batch Normalization, and Weight Decay are used to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdff920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Environment Setup (for Google Colab) ---\n",
    "from google.colab import drive, userdata\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "print(\"--- Setting up Environment ---\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# IMPORTANT: Make sure this path matches your project folder in Google Drive\n",
    "PROJECT_PATH = '/content/drive/MyDrive/Airbnb_Price_Project'\n",
    "os.chdir(PROJECT_PATH)\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# --- Standard and Third-Party Library Imports ---\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Imports from Custom Project Scripts ---\n",
    "print(\"\\n--- Importing Custom Modules ---\")\n",
    "from config import config\n",
    "from data_processing import load_and_split_data, FeatureProcessor, create_dataloaders, AirbnbPriceDataset\n",
    "from model import BaselineModel\n",
    "from train import train_model, evaluate_model\n",
    "from inference import run_inference\n",
    "\n",
    "print(\"\\nSetup and imports complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36de2fbf",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "We begin by loading the dataset and performing our custom stratified group split. This method ensures that all records for a single listing (`listing_id`) are confined to either the training or the validation set, which is crucial for preventing data leakage and obtaining a reliable performance estimate.\n",
    "\n",
    "Once split, we instantiate and `fit` our `FeatureProcessor` exclusively on the training data. This learns the necessary vocabularies and scaling parameters, which are then used to `transform` both the training and validation sets into numerical tensors ready for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158b0a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split the data\n",
    "train_df, val_df, neighborhood_log_means, train_ids, val_ids = load_and_split_data(config)\n",
    "\n",
    "# Instantiate and fit the feature processor on the training data\n",
    "processor = FeatureProcessor(config)\n",
    "processor.fit(train_df)\n",
    "\n",
    "# Transform both datasets into feature dictionaries\n",
    "train_features = processor.transform(train_df, neighborhood_log_means)\n",
    "val_features = processor.transform(val_df, neighborhood_log_means)\n",
    "\n",
    "# Create the PyTorch DataLoaders\n",
    "train_loader, val_loader = create_dataloaders(train_features, val_features, config)\n",
    "\n",
    "print(\"\\nData pipeline complete. DataLoaders are ready for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36442d75",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "\n",
    "Here, we instantiate our `BaselineModel` from the `model.py` script. The architecture is defined by the parameters in our central `config` file.\n",
    "\n",
    "We then define the `AdamW` optimizer, which is a robust choice for deep learning models. Crucially, we include a `weight_decay` parameter, which applies L2 regularization to help prevent overfitting. Finally, we set up a `ReduceLROnPlateau` scheduler, which will automatically decrease the learning rate if the validation performance stagnates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dab76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the baseline model\n",
    "model = BaselineModel(processor, config)\n",
    "model.to(config['DEVICE'])\n",
    "\n",
    "# Instantiate the optimizer with weight decay for regularization\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config['LEARNING_RATE'],\n",
    "    weight_decay=config['WEIGHT_DECAY']\n",
    ")\n",
    "\n",
    "# Instantiate the learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=config['SCHEDULER_FACTOR'],\n",
    "    patience=config['SCHEDULER_PATIENCE']\n",
    ")\n",
    "\n",
    "print(f\"BaselineModel, AdamW optimizer, and ReduceLROnPlateau scheduler have been initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa37ca9",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "We now pass all the prepared components—the model, data loaders, optimizer, and scheduler—to our reusable `train_model` function from the `train.py` script. This function encapsulates the entire training process:\n",
    "- It iterates through epochs.\n",
    "- It performs forward and backward passes.\n",
    "- It calculates validation metrics after each epoch.\n",
    "- It implements early stopping to halt training if the validation MAPE fails to improve, preventing overfitting and saving time.\n",
    "- It returns the best performing model state and a history of the training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281a0a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model, history_df = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c5ced2",
   "metadata": {},
   "source": [
    "## Final Evaluation\n",
    "\n",
    "With the best model checkpoint from the training process automatically loaded, we perform a final, definitive evaluation on both the training and validation sets. This provides the final performance metrics (RMSE and MAPE) that we will use to compare this baseline against other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4211e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Final Model Evaluation ---\")\n",
    "final_train_mse, final_train_mape = evaluate_model(trained_model, train_loader, config['DEVICE'])\n",
    "final_val_mse, final_val_mape = evaluate_model(trained_model, val_loader, config['DEVICE'])\n",
    "\n",
    "final_metrics = {\n",
    "    \"train_rmse\": np.sqrt(final_train_mse),\n",
    "    \"train_mape\": final_train_mape,\n",
    "    \"val_rmse\": np.sqrt(final_val_mse),\n",
    "    \"val_mape\": final_val_mape\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"{'Final Baseline Performance Metrics':^50}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Train RMSE:      {final_metrics['train_rmse']:.4f}\")\n",
    "print(f\"Validation RMSE: {final_metrics['val_rmse']:.4f}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Train MAPE:      {final_metrics['train_mape'] * 100:.2f}%\")\n",
    "print(f\"Validation MAPE: {final_metrics['val_mape'] * 100:.2f}%\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87afeda",
   "metadata": {},
   "source": [
    "## Generating Predictions for the Full Dataset\n",
    "\n",
    "For our final analysis and to provide data for potential applications, we need predictions for every listing for every month of the year. We create a \"full panel\" dataset by taking all unique listings and creating a row for each of the 12 months. We then run our trained model on this complete panel to generate a `predicted_price` for every entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466e68d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_app_dataset import create_full_panel_dataset # Re-use the panel creation logic\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(\"--- Preparing full panel dataset for inference ---\")\n",
    "\n",
    "# 1. Load the original raw data to get all listings\n",
    "raw_df = pd.read_parquet(f\"./{config['CITY']}_dataset_oct_20.parquet\")\n",
    "panel_df = create_full_panel_dataset(raw_df, train_ids, val_ids)\n",
    "\n",
    "# 2. Transform features for the entire panel using the fitted processor\n",
    "panel_features = processor.transform(panel_df, neighborhood_log_means)\n",
    "\n",
    "# 3. Create a DataLoader for the panel\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['TEXT_MODEL_NAME'], use_fast=True)\n",
    "panel_dataset = AirbnbPriceDataset(panel_features, tokenizer)\n",
    "panel_loader = DataLoader(\n",
    "    panel_dataset,\n",
    "    batch_size=config['VALIDATION_BATCH_SIZE'],\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# 4. Run inference to get predictions\n",
    "predictions_df = run_inference(trained_model, panel_loader, config['DEVICE'])\n",
    "\n",
    "# 5. Combine the panel data with the predictions\n",
    "final_predictions_df = pd.concat([panel_df, predictions_df], axis=1)\n",
    "\n",
    "print(\"\\nInference complete. Final predictions DataFrame created.\")\n",
    "display(final_predictions_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d49517",
   "metadata": {},
   "source": [
    "## Save Artifacts\n",
    "\n",
    "Finally, we save all the essential outputs of this notebook. This includes:\n",
    "- The trained model's state dictionary (`.pt` file).\n",
    "- The fitted `FeatureProcessor` instance.\n",
    "- A dictionary containing the final performance metrics.\n",
    "- The complete DataFrame with predictions for every listing-month.\n",
    "\n",
    "These artifacts ensure our work is reproducible and can be easily loaded into our final analysis notebook (`04_results_and_analysis.ipynb`) without needing to retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2901253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "print(\"--- Saving all artifacts ---\")\n",
    "\n",
    "# 1. Define paths\n",
    "timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "artifacts_dir = os.path.join(config['DRIVE_SAVE_PATH'], f\"baseline_{timestamp}\")\n",
    "os.makedirs(artifacts_dir, exist_ok=True)\n",
    "\n",
    "model_save_path = os.path.join(artifacts_dir, \"baseline_model.pt\")\n",
    "processor_save_path = os.path.join(artifacts_dir, \"feature_processor.pkl\")\n",
    "metrics_save_path = os.path.join(artifacts_dir, \"final_metrics.pkl\")\n",
    "predictions_save_path = os.path.join(artifacts_dir, \"baseline_model_predictions.parquet\")\n",
    "\n",
    "# 2. Save the model state and metrics\n",
    "torch.save({\n",
    "    'model_state_dict': trained_model.state_dict(),\n",
    "    'final_metrics': final_metrics\n",
    "}, model_save_path)\n",
    "print(f\"Model and metrics saved to: {model_save_path}\")\n",
    "\n",
    "# 3. Save the feature processor\n",
    "with open(processor_save_path, 'wb') as f:\n",
    "    pickle.dump(processor, f)\n",
    "print(f\"Feature processor saved to: {processor_save_path}\")\n",
    "\n",
    "# 4. Save the predictions DataFrame\n",
    "final_predictions_df.to_parquet(predictions_save_path, index=False)\n",
    "print(f\"Predictions DataFrame saved to: {predictions_save_path}\")\n",
    "\n",
    "print(\"\\nAll baseline model artifacts have been saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
