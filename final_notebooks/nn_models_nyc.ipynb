{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arvindsuresh-math/Fall-2025-Team-Big-Data/blob/main/final_notebooks/nn_models_nyc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d71cad3",
      "metadata": {
        "id": "6d71cad3"
      },
      "source": [
        "# 1. Baseline Neural Network Model\n",
        "\n",
        "### Objective\n",
        "\n",
        "In this notebook, we train and evaluate a fully-connected deep learning model. This model will serve as a robust performance baseline against which we can compare our more complex, interpretable `AdditiveModel`.\n",
        "\n",
        "The architecture is a standard Multi-Layer Perceptron (MLP) that takes all available features—including location, size, quality, and text embeddings—concatenates them into a single vector, and processes them through several layers to predict the final log-price deviation. Regularization techniques like Dropout, Batch Normalization, and Weight Decay are used to prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8fdff920",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fdff920",
        "outputId": "959d45d7-5b21-4c2e-bdf9-15b709539e78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Current working directory: /content/drive/MyDrive/Airbnb_Price_Project\n"
          ]
        }
      ],
      "source": [
        "# --- Mount Google Drive ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Change Directory to Project Folder ---\n",
        "import os\n",
        "\n",
        "# IMPORTANT: Make sure this path matches the location of your project folder in Google Drive\n",
        "PROJECT_PATH = '/content/drive/MyDrive/Airbnb_Price_Project'\n",
        "os.chdir(PROJECT_PATH)\n",
        "print(f\"Current working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Hugging Face Authentication ---\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "print(\"\\nAttempting Hugging Face login...\")\n",
        "try:\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    login(token=HF_TOKEN)\n",
        "    print(\"Hugging Face login successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not log in. Please ensure 'HF_TOKEN' is a valid secret. Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6ZLgwn3RDQf",
        "outputId": "0e82f758-2a4c-404f-e75b-2a5fcaa19a2c"
      },
      "id": "Y6ZLgwn3RDQf",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Attempting Hugging Face login...\n",
            "Hugging Face login successful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Installing required packages ---\")\n",
        "!pip install -q pandas pyarrow sentence-transformers scikit-learn torch tqdm transformers matplotlib seaborn\n",
        "\n",
        "print(\"Package installation complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LObqTjWREzL",
        "outputId": "9febfc26-f338-473a-bf3d-c3043e5482bc"
      },
      "id": "7LObqTjWREzL",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Installing required packages ---\n",
            "Package installation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# --- Custom Project Scripts ---\n",
        "from config import config\n",
        "from data_processing import load_and_split_data, FeatureProcessor, create_dataloaders\n",
        "# Import BOTH model classes and the dataset\n",
        "from model import BaselineModel, AdditiveModel, AirbnbPriceDataset\n",
        "from train import train_model\n",
        "# Import BOTH inference functions\n",
        "from inference import run_inference\n",
        "from build_app_dataset import build_dataset, create_full_panel_dataset"
      ],
      "metadata": {
        "id": "e9DpwNxNVGqY"
      },
      "id": "e9DpwNxNVGqY",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed: int):\n",
        "    \"\"\"\n",
        "    Sets random seeds for numpy, torch, and Python's random module to ensure\n",
        "    reproducible results across runs.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        # These settings are needed for full determinism with CUDA\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "    print(f\"All random seeds set to {seed}.\")\n",
        "\n",
        "set_seed(config[\"SEED\"])\n",
        "config[\"CITY\"] = \"nyc\"\n",
        "print(\"-\"*60)\n",
        "print(\"Current configuration:\")\n",
        "for key, value in config.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ihqv5Ai4Rkqv",
        "outputId": "b9e3cfc6-df55-4887-a2db-f17139ea1477"
      },
      "id": "Ihqv5Ai4Rkqv",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All random seeds set to 42.\n",
            "------------------------------------------------------------\n",
            "Current configuration:\n",
            "CITY: nyc\n",
            "DEVICE: cuda\n",
            "DRIVE_SAVE_PATH: /content/drive/MyDrive/Airbnb_Price_Project/artifacts/\n",
            "TEXT_MODEL_NAME: BAAI/bge-small-en-v1.5\n",
            "VAL_SIZE: 0.05\n",
            "SEED: 42\n",
            "BATCH_SIZE: 256\n",
            "VALIDATION_BATCH_SIZE: 512\n",
            "LEARNING_RATE: 0.001\n",
            "TRANSFORMER_LEARNING_RATE: 1e-05\n",
            "N_EPOCHS: 100\n",
            "WEIGHT_DECAY: 0.0001\n",
            "DROPOUT_RATE: 0.2\n",
            "GEO_EMBEDDING_DIM: 32\n",
            "HIDDEN_LAYERS_LOCATION: [32, 16]\n",
            "HIDDEN_LAYERS_SIZE_CAPACITY: [32, 16]\n",
            "HIDDEN_LAYERS_QUALITY: [32, 16]\n",
            "HIDDEN_LAYERS_AMENITIES: [64, 32]\n",
            "HIDDEN_LAYERS_DESCRIPTION: [64, 32]\n",
            "HIDDEN_LAYERS_SEASONALITY: [16]\n",
            "EARLY_STOPPING_PATIENCE: 3\n",
            "EARLY_STOPPING_MIN_DELTA: 0.0\n",
            "SCHEDULER_PATIENCE: 2\n",
            "SCHEDULER_FACTOR: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36de2fbf",
      "metadata": {
        "id": "36de2fbf"
      },
      "source": [
        "## Data Loading and Preprocessing\n",
        "\n",
        "We begin by loading the dataset and performing our custom stratified group split. This method ensures that all records for a single listing (`listing_id`) are confined to either the training or the validation set, which is crucial for preventing data leakage and obtaining a reliable performance estimate.\n",
        "\n",
        "Once split, we instantiate and `fit` our `FeatureProcessor` exclusively on the training data. This learns the necessary vocabularies and scaling parameters, which are then used to `transform` both the training and validation sets into numerical tensors ready for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "158b0a7a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "158b0a7a",
        "outputId": "6aee42db-47e6-4102-dcb6-e9b142300401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data split: 121,187 train records, 6,403 validation records.\n",
            "\n",
            "Data pipeline complete. DataLoaders are ready for training.\n"
          ]
        }
      ],
      "source": [
        "# Load and split the data\n",
        "train_df, val_df, neighborhood_log_means, train_ids, val_ids = load_and_split_data(config)\n",
        "\n",
        "# Instantiate and fit the feature processor on the training data\n",
        "processor = FeatureProcessor(config)\n",
        "processor.fit(train_df)\n",
        "\n",
        "# Transform both datasets into feature dictionaries\n",
        "train_features = processor.transform(train_df, neighborhood_log_means)\n",
        "val_features = processor.transform(val_df, neighborhood_log_means)\n",
        "\n",
        "# Create the PyTorch DataLoaders\n",
        "train_loader, val_loader = create_dataloaders(train_features, val_features, config)\n",
        "\n",
        "print(\"\\nData pipeline complete. DataLoaders are ready for training.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36442d75",
      "metadata": {
        "id": "36442d75"
      },
      "source": [
        "---\n",
        "# Part 1: Baseline Model\n",
        "---\n",
        "\n",
        "First, we train the `BaselineModel`. This involves initializing the model and a standard optimizer, running the training loop, and saving all the necessary artifacts for later analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "68dab76a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68dab76a",
        "outputId": "f85106c2-1d5f-44f4-a574-9d1219428ab1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BaselineModel and its optimizer/scheduler have been initialized.\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the baseline model\n",
        "baseline_model = BaselineModel(processor, config)\n",
        "baseline_model.to(config['DEVICE'])\n",
        "\n",
        "# Instantiate the optimizer with weight decay for regularization\n",
        "baseline_optimizer = optim.AdamW(\n",
        "    baseline_model.parameters(),\n",
        "    lr=config['LEARNING_RATE'],\n",
        "    weight_decay=config['WEIGHT_DECAY']\n",
        ")\n",
        "\n",
        "# Instantiate the learning rate scheduler\n",
        "baseline_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    baseline_optimizer,\n",
        "    mode='min',\n",
        "    factor=config['SCHEDULER_FACTOR'],\n",
        "    patience=config['SCHEDULER_PATIENCE']\n",
        ")\n",
        "\n",
        "print(f\"BaselineModel and its optimizer/scheduler have been initialized.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "281a0a04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "281a0a04",
        "outputId": "6c46226e-3127-4867-92b8-e9be69a41392"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Training for BaselineModel on NYC ---\n",
            "Epoch |     Time |   Train RMSE | Train MAPE (%) |   Val RMSE | Val MAPE (%) | MAPE Gap (%) | Patience\n",
            "------------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    1 | 00:01:43 |       0.3627 |          29.82 |     0.3505 |        29.14 |        -0.68 |        0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    2 | 00:03:26 |       0.3203 |          26.06 |     0.3362 |        27.46 |         1.40 |        0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    3 | 00:05:09 |       0.3008 |          24.46 |     0.3305 |        27.89 |         3.43 |        0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    4 | 00:06:52 |       0.2877 |          23.31 |     0.3336 |        28.24 |         4.93 |        1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    5 | 00:08:34 |       0.2770 |          22.40 |     0.3328 |        27.86 |         5.46 |        2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    6 | 00:10:17 |       0.2641 |          21.28 |     0.3274 |        26.86 |         5.58 |        3\n",
            "--- Early Stopping Triggered (MAPE Gap exceeded 4% for 3 epochs) ---\n",
            "\n",
            "--- Training Complete ---\n",
            "Loading best model state from file with Train MAPE: 24.46% (and MAPE Gap: 3.43%)\n"
          ]
        }
      ],
      "source": [
        "trained_baseline_model, baseline_history_df = train_model(\n",
        "    model=baseline_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=baseline_optimizer,\n",
        "    scheduler=baseline_scheduler,\n",
        "    config=config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Performance Metrics\n",
        "\n",
        "With the training complete, the `train_model` function has returned the model object with the weights from its best-performing epoch, defined by our custom criteria (lowest `train_mape` while the validation/train MAPE gap is under 4%).\n",
        "\n",
        "To get the definitive performance scores for our saved model, we now query the `history_df` using the **exact same logic** to identify the best epoch and extract its corresponding metrics."
      ],
      "metadata": {
        "id": "pMxszD4cs6WA"
      },
      "id": "pMxszD4cs6WA"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ff4211e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff4211e7",
        "outputId": "dc2f105f-41af-44c3-94cd-6b8acdae6090"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "          Final Baseline Model Performance Metrics          \n",
            "(Extracted from Best Epoch: 3)\n",
            "============================================================\n",
            "Train RMSE:      0.3008\n",
            "Validation RMSE: 0.3305\n",
            "------------------------------------------------------------\n",
            "Train MAPE:      24.46%\n",
            "Validation MAPE: 27.89%\n",
            "MAPE Gap:        3.43%\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# 1. Filter the history to find all epochs that satisfy the gap constraint (< 4%)\n",
        "valid_epochs_df = baseline_history_df[baseline_history_df['mape_gap'] < 0.04]\n",
        "\n",
        "if not valid_epochs_df.empty:\n",
        "    # 2. From these valid epochs, find the index of the one with the lowest train MAPE\n",
        "    best_epoch_idx = valid_epochs_df['train_mape'].idxmin()\n",
        "\n",
        "    # 3. Select the entire row of metrics from that best epoch\n",
        "    best_metrics_series = baseline_history_df.loc[best_epoch_idx]\n",
        "\n",
        "    # 4. Convert the pandas Series to a dictionary\n",
        "    final_baseline_metrics = best_metrics_series.to_dict()\n",
        "\n",
        "    # --- Print a clear summary for verification ---\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"{'Final Baseline Model Performance Metrics':^60}\")\n",
        "    print(f\"(Extracted from Best Epoch: {int(final_baseline_metrics['epoch']) + 1})\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Train RMSE:      {final_baseline_metrics['train_rmse']:.4f}\")\n",
        "    print(f\"Validation RMSE: {final_baseline_metrics['val_rmse']:.4f}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"Train MAPE:      {final_baseline_metrics['train_mape'] * 100:.2f}%\")\n",
        "    print(f\"Validation MAPE: {final_baseline_metrics['val_mape'] * 100:.2f}%\")\n",
        "    print(f\"MAPE Gap:        {final_baseline_metrics['mape_gap'] * 100:.2f}%\")\n",
        "    print(\"=\" * 60)\n",
        "else:\n",
        "    print(\"ERROR: No valid epochs found that met the <4% MAPE gap criterion.\")\n",
        "    # Create a dummy dictionary to prevent the next cell from crashing\n",
        "    final_baseline_metrics = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "466e68d2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "466e68d2",
        "outputId": "7b69e0cc-b7f8-4554-9d83-776bca6a30ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Preparing full panel dataset for baseline inference ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running Inference: 100%|██████████| 272/272 [06:10<00:00,  1.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Saving all baseline artifacts ---\n",
            "Baseline artifacts for NYC successfully saved in folder: /content/drive/MyDrive/Airbnb_Price_Project/artifacts/nyc_baseline_20251105_165022\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Preparing full panel dataset for baseline inference ---\")\n",
        "raw_df = pd.read_parquet(f\"./{config['CITY']}_dataset_oct_20.parquet\")\n",
        "panel_df = create_full_panel_dataset(raw_df, train_ids, val_ids)\n",
        "panel_features = processor.transform(panel_df, neighborhood_log_means)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config['TEXT_MODEL_NAME'], use_fast=True)\n",
        "panel_dataset = AirbnbPriceDataset(panel_features, tokenizer)\n",
        "panel_loader = DataLoader(panel_dataset, batch_size=config['VALIDATION_BATCH_SIZE'], shuffle=False)\n",
        "predictions_df = run_inference(trained_baseline_model, panel_loader, config['DEVICE'])\n",
        "final_predictions_df = pd.concat([panel_df, predictions_df], axis=1)\n",
        "\n",
        "print(\"\\n--- Saving all baseline artifacts ---\")\n",
        "timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
        "# UPDATED: The directory name now includes the city\n",
        "artifacts_dir = os.path.join(config['DRIVE_SAVE_PATH'], f\"{config['CITY']}_baseline_{timestamp}\")\n",
        "os.makedirs(artifacts_dir, exist_ok=True)\n",
        "\n",
        "# UPDATED: All filenames are now prefixed with the city name\n",
        "model_save_path = os.path.join(artifacts_dir, f\"{config['CITY']}_baseline_model.pt\")\n",
        "processor_save_path = os.path.join(artifacts_dir, f\"{config['CITY']}_feature_processor.pkl\")\n",
        "predictions_save_path = os.path.join(artifacts_dir, f\"{config['CITY']}_baseline_model_predictions.parquet\")\n",
        "\n",
        "# Save model, processor, and predictions\n",
        "torch.save({\n",
        "    'model_state_dict': trained_baseline_model.state_dict(),\n",
        "    'final_metrics': final_baseline_metrics\n",
        "}, model_save_path)\n",
        "with open(processor_save_path, 'wb') as f:\n",
        "    pickle.dump(processor, f)\n",
        "final_predictions_df.to_parquet(predictions_save_path, index=False)\n",
        "\n",
        "print(f\"Baseline artifacts for {config['CITY'].upper()} successfully saved in folder: {artifacts_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93d49517",
      "metadata": {
        "id": "93d49517"
      },
      "source": [
        "---\n",
        "# Part 2: Additive Model\n",
        "---\n",
        "\n",
        "Now, we proceed to train the `AdditiveModel`. We reuse the exact same data loaders to ensure a fair comparison. The key difference in this section is the optimizer setup, which uses a lower learning rate for the pre-trained text transformer to enable effective fine-tuning. After training, we run the specialized `build_dataset` script to generate the enriched data artifact for our Streamlit application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2901253b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2901253b",
        "outputId": "bf272f9f-fdef-46a1-bb4f-3410cf8850bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdditiveModel and its optimizer/scheduler have been initialized.\n"
          ]
        }
      ],
      "source": [
        "additive_model = AdditiveModel(processor, config)\n",
        "additive_model.to(config['DEVICE'])\n",
        "\n",
        "# Create parameter groups for differential learning rates\n",
        "transformer_params = additive_model.text_transformer.parameters()\n",
        "other_params = [p for n, p in additive_model.named_parameters() if 'text_transformer' not in n]\n",
        "\n",
        "# Instantiate the optimizer with two parameter groups\n",
        "additive_optimizer = optim.AdamW([\n",
        "    {'params': other_params, 'lr': config['LEARNING_RATE'], 'weight_decay': config['WEIGHT_DECAY']},\n",
        "    {'params': transformer_params, 'lr': config['TRANSFORMER_LEARNING_RATE']}\n",
        "])\n",
        "\n",
        "additive_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    additive_optimizer, mode='min', factor=config['SCHEDULER_FACTOR'], patience=config['SCHEDULER_PATIENCE']\n",
        ")\n",
        "\n",
        "print(f\"AdditiveModel and its optimizer/scheduler have been initialized.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_additive_model, additive_history_df = train_model(\n",
        "    model=additive_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=additive_optimizer,\n",
        "    scheduler=additive_scheduler,\n",
        "    config=config\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPxT-a4Miqj5",
        "outputId": "b308500e-4e65-439c-c54a-337114a71e8e"
      },
      "id": "EPxT-a4Miqj5",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Training for AdditiveModel on NYC ---\n",
            "Epoch |     Time |   Train RMSE | Train MAPE (%) |   Val RMSE | Val MAPE (%) | MAPE Gap (%) | Patience\n",
            "------------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    1 | 00:01:41 |       0.4566 |          39.40 |     0.3556 |        28.50 |       -10.90 |        0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    2 | 00:03:22 |       0.3370 |          27.60 |     0.3452 |        28.51 |         0.90 |        0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    3 | 00:05:03 |       0.3176 |          25.94 |     0.3442 |        27.76 |         1.82 |        0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    4 | 00:06:46 |       0.3059 |          24.96 |     0.3465 |        27.30 |         2.34 |        0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    5 | 00:08:26 |       0.2974 |          24.15 |     0.3407 |        27.69 |         3.54 |        0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    6 | 00:10:09 |       0.2905 |          23.63 |     0.3449 |        27.30 |         3.67 |        0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    7 | 00:11:50 |       0.2848 |          23.12 |     0.3384 |        27.43 |         4.31 |        1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    8 | 00:13:32 |       0.2760 |          22.35 |     0.3395 |        27.07 |         4.73 |        2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    9 | 00:15:13 |       0.2732 |          22.08 |     0.3416 |        27.70 |         5.62 |        3\n",
            "--- Early Stopping Triggered (MAPE Gap exceeded 4% for 3 epochs) ---\n",
            "\n",
            "--- Training Complete ---\n",
            "Loading best model state from file with Train MAPE: 23.63% (and MAPE Gap: 3.67%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Filter the history to find all epochs that satisfy the gap constraint (< 4%)\n",
        "valid_epochs_df = additive_history_df[additive_history_df['mape_gap'] < 0.04]\n",
        "\n",
        "if not valid_epochs_df.empty:\n",
        "    # 2. From these valid epochs, find the index of the one with the lowest train MAPE\n",
        "    best_epoch_idx = valid_epochs_df['train_mape'].idxmin()\n",
        "\n",
        "    # 3. Select the entire row of metrics from that best epoch\n",
        "    best_metrics_series = additive_history_df.loc[best_epoch_idx]\n",
        "\n",
        "    # 4. Convert the pandas Series to a dictionary\n",
        "    final_additive_metrics = best_metrics_series.to_dict()\n",
        "\n",
        "    # --- Print a clear summary for verification ---\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"{'Final Additive Model Performance Metrics':^60}\")\n",
        "    print(f\"(Extracted from Best Epoch: {int(final_additive_metrics['epoch']) + 1})\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Train RMSE:      {final_additive_metrics['train_rmse']:.4f}\")\n",
        "    print(f\"Validation RMSE: {final_additive_metrics['val_rmse']:.4f}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"Train MAPE:      {final_additive_metrics['train_mape'] * 100:.2f}%\")\n",
        "    print(f\"Validation MAPE: {final_additive_metrics['val_mape'] * 100:.2f}%\")\n",
        "    print(f\"MAPE Gap:        {final_additive_metrics['mape_gap'] * 100:.2f}%\")\n",
        "    print(\"=\" * 60)\n",
        "else:\n",
        "    print(\"ERROR: No valid epochs found for the Additive Model that met the <4% MAPE gap criterion.\")\n",
        "    # Create a dummy dictionary to prevent the next cell from crashing\n",
        "    final_additive_metrics = {}"
      ],
      "metadata": {
        "id": "TRI0CKCkis9h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cbc1e28-c505-4207-b4ce-44e6fa5b88f9"
      },
      "id": "TRI0CKCkis9h",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "          Final Additive Model Performance Metrics          \n",
            "(Extracted from Best Epoch: 6)\n",
            "============================================================\n",
            "Train RMSE:      0.2905\n",
            "Validation RMSE: 0.3449\n",
            "------------------------------------------------------------\n",
            "Train MAPE:      23.63%\n",
            "Validation MAPE: 27.30%\n",
            "MAPE Gap:        3.67%\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Building and saving final application dataset ---\")\n",
        "build_dataset(\n",
        "    model=trained_additive_model,\n",
        "    processor=processor,\n",
        "    config=config,\n",
        "    train_ids=train_ids,\n",
        "    val_ids=val_ids\n",
        ")"
      ],
      "metadata": {
        "id": "tZdG1nCxuLAw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3943e89-ca0f-48f0-f3cd-6fb2ce6c06ce"
      },
      "id": "tZdG1nCxuLAw",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Building and saving final application dataset ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running Detailed Inference: 100%|██████████| 272/272 [02:39<00:00,  1.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Successfully created application database at: /content/drive/MyDrive/Airbnb_Price_Project/artifacts/app_data/nyc_app_database.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We also save the core model artifacts for reproducibility.\n",
        "print(\"\\n--- Saving core additive model artifacts ---\")\n",
        "timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
        "# UPDATED: The directory name now includes the city\n",
        "artifacts_dir = os.path.join(config['DRIVE_SAVE_PATH'], f\"{config['CITY']}_additive_{timestamp}\")\n",
        "os.makedirs(artifacts_dir, exist_ok=True)\n",
        "\n",
        "# UPDATED: All filenames are now prefixed with the city name\n",
        "model_save_path = os.path.join(artifacts_dir, f\"{config['CITY']}_additive_model.pt\")\n",
        "processor_save_path = os.path.join(artifacts_dir, f\"{config['CITY']}_feature_processor.pkl\")\n",
        "\n",
        "# Save model and processor\n",
        "torch.save({\n",
        "    'model_state_dict': trained_additive_model.state_dict(),\n",
        "    'final_metrics': final_additive_metrics\n",
        "}, model_save_path)\n",
        "with open(processor_save_path, 'wb') as f:\n",
        "    pickle.dump(processor, f)\n",
        "\n",
        "print(f\"Core additive artifacts for {config['CITY'].upper()} successfully saved in folder: {artifacts_dir}\")\n",
        "print(\"\\n\\nNotebook complete.\")"
      ],
      "metadata": {
        "id": "ILKipFx2kPgx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2866856a-315f-453a-d263-1973dae203c5"
      },
      "id": "ILKipFx2kPgx",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Saving core additive model artifacts ---\n",
            "Core additive artifacts for NYC successfully saved in folder: /content/drive/MyDrive/Airbnb_Price_Project/artifacts/nyc_additive_20251105_170821\n",
            "\n",
            "\n",
            "Notebook complete.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}